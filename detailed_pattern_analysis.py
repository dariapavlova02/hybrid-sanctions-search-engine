#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Set, Any
import re
from collections import Counter

# Add path to src for imports
sys.path.append(str(Path(__file__).parent / "src"))

from ai_service.utils.logging_config import get_logger


class DetailedPatternAnalyzer:
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def load_patterns(self, patterns_file: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(patterns_file, 'r', encoding='utf-8') as f:
                return [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {patterns_file}: {e}")
            return []
    
    def analyze_short_patterns(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        short_patterns = [p for p in patterns if len(p.strip()) < 3]
        
        analysis = {
            'total_short': len(short_patterns),
            'examples': short_patterns[:20],
            'by_length': Counter(len(p) for p in short_patterns),
            'suspicious': []
        }
        
        # –ò—â–µ–º –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        for pattern in short_patterns:
            if pattern.isdigit() and len(pattern) == 1:
                analysis['suspicious'].append(f"Single digit: '{pattern}'")
            elif pattern in ['–∞', '–æ', '–µ', '–∏', '—É', '—è', '—é', '—ë', '—ã', '—ç']:
                analysis['suspicious'].append(f"Single vowel: '{pattern}'")
            elif pattern in ['–±', '–≤', '–≥', '–¥', '–∂', '–∑', '–∫', '–ª', '–º', '–Ω', '–ø', '—Ä', '—Å', '—Ç', '—Ñ', '—Ö', '—Ü', '—á', '—à', '—â']:
                analysis['suspicious'].append(f"Single consonant: '{pattern}'")
        
        return analysis
    
    def analyze_long_patterns(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        long_patterns = [p for p in patterns if len(p.strip()) > 200]
        
        analysis = {
            'total_long': len(long_patterns),
            'examples': [p[:100] + "..." for p in long_patterns[:10]],
            'length_distribution': Counter(len(p) for p in long_patterns),
            'avg_length': sum(len(p) for p in long_patterns) / len(long_patterns) if long_patterns else 0,
            'max_length': max(len(p) for p in long_patterns) if long_patterns else 0
        }
        
        return analysis
    
    def analyze_duplicates(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã"""
        pattern_counts = Counter(patterns)
        duplicates = {pattern: count for pattern, count in pattern_counts.items() if count > 1}
        
        analysis = {
            'total_duplicates': len(duplicates),
            'duplicate_examples': dict(list(duplicates.items())[:20]),
            'most_common': pattern_counts.most_common(10),
            'duplicate_frequency': Counter(count for count in pattern_counts.values() if count > 1)
        }
        
        return analysis
    
    def analyze_whitespace_issues(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏"""
        issues = {
            'leading_spaces': [],
            'trailing_spaces': [],
            'multiple_spaces': [],
            'tabs': [],
            'newlines': []
        }
        
        for i, pattern in enumerate(patterns):
            if pattern.startswith(' '):
                issues['leading_spaces'].append(f"Pattern {i}: '{pattern[:50]}...'")
            if pattern.endswith(' '):
                issues['trailing_spaces'].append(f"Pattern {i}: '{pattern[:50]}...'")
            if '  ' in pattern:
                issues['multiple_spaces'].append(f"Pattern {i}: '{pattern[:50]}...'")
            if '\t' in pattern:
                issues['tabs'].append(f"Pattern {i}: '{pattern[:50]}...'")
            if '\n' in pattern:
                issues['newlines'].append(f"Pattern {i}: '{pattern[:50]}...'")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        for key in issues:
            issues[key] = issues[key][:10]
        
        return issues
    
    def analyze_case_issues(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º"""
        issues = {
            'mixed_case_words': [],
            'inconsistent_capitalization': [],
            'all_uppercase': [],
            'all_lowercase': []
        }
        
        for i, pattern in enumerate(patterns):
            words = pattern.split()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä –≤ —Å–ª–æ–≤–∞—Ö
            for word in words:
                if len(word) > 1 and word[0].islower() and any(c.isupper() for c in word[1:]):
                    issues['mixed_case_words'].append(f"Pattern {i}: word '{word}' in '{pattern[:50]}...'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
            if len(words) > 1:
                first_word_upper = words[0][0].isupper() if words[0] else False
                other_words_upper = [w[0].isupper() for w in words[1:] if w]
                
                if first_word_upper and not all(other_words_upper):
                    issues['inconsistent_capitalization'].append(f"Pattern {i}: '{pattern[:50]}...'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ/—Å—Ç—Ä–æ—á–Ω—ã–µ
            if pattern.isupper() and len(pattern) > 3:
                issues['all_uppercase'].append(f"Pattern {i}: '{pattern[:50]}...'")
            elif pattern.islower() and len(pattern) > 3:
                issues['all_lowercase'].append(f"Pattern {i}: '{pattern[:50]}...'")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        for key in issues:
            issues[key] = issues[key][:10]
        
        return issues
    
    def analyze_encoding_issues(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π"""
        issues = {
            'unicode_errors': [],
            'suspicious_chars': [],
            'encoding_inconsistencies': []
        }
        
        for i, pattern in enumerate(patterns[:1000]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 1000
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
                encoded = pattern.encode('utf-8')
                decoded = encoded.decode('utf-8')
                if decoded != pattern:
                    issues['encoding_inconsistencies'].append(f"Pattern {i}: roundtrip failed")
            except UnicodeError as e:
                issues['unicode_errors'].append(f"Pattern {i}: {e}")
            
            # –ò—â–µ–º –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            suspicious_chars = re.findall(r'[^\x00-\x7F–ê-–Ø–∞-—è–Å—ë–Ü—ñ–á—ó–Ñ—î“ê“ë\s\.\-\'\"\(\)\d]', pattern)
            if suspicious_chars:
                issues['suspicious_chars'].append(f"Pattern {i}: chars {set(suspicious_chars)} in '{pattern[:50]}...'")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        for key in issues:
            issues[key] = issues[key][:10]
        
        return issues
    
    def analyze_pattern_quality(self, patterns: List[str]) -> Dict[str, Any]:
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        quality_issues = {
            'empty_patterns': [],
            'whitespace_only': [],
            'single_char_non_alpha': [],
            'repeated_chars': [],
            'suspicious_patterns': []
        }
        
        for i, pattern in enumerate(patterns):
            if not pattern.strip():
                quality_issues['empty_patterns'].append(f"Pattern {i}: empty")
            elif pattern.isspace():
                quality_issues['whitespace_only'].append(f"Pattern {i}: whitespace only")
            elif len(pattern) == 1 and not pattern.isalnum():
                quality_issues['single_char_non_alpha'].append(f"Pattern {i}: '{pattern}'")
            elif len(set(pattern)) == 1 and len(pattern) > 2:
                quality_issues['repeated_chars'].append(f"Pattern {i}: '{pattern}'")
            
            # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if re.match(r'^[0-9]+$', pattern) and len(pattern) > 10:
                quality_issues['suspicious_patterns'].append(f"Pattern {i}: very long number '{pattern[:20]}...'")
            elif re.match(r'^[a-zA-Z]+$', pattern) and len(pattern) > 50:
                quality_issues['suspicious_patterns'].append(f"Pattern {i}: very long word '{pattern[:20]}...'")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        for key in quality_issues:
            quality_issues[key] = quality_issues[key][:10]
        
        return quality_issues
    
    def generate_detailed_report(self, patterns_file: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç"""
        self.logger.info("–ù–∞—á–∏–Ω–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
        
        patterns = self.load_patterns(patterns_file)
        if not patterns:
            return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
        short_analysis = self.analyze_short_patterns(patterns)
        long_analysis = self.analyze_long_patterns(patterns)
        duplicate_analysis = self.analyze_duplicates(patterns)
        whitespace_analysis = self.analyze_whitespace_issues(patterns)
        case_analysis = self.analyze_case_issues(patterns)
        encoding_analysis = self.analyze_encoding_issues(patterns)
        quality_analysis = self.analyze_pattern_quality(patterns)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = f"""
=== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–ê–¢–¢–ï–†–ù–û–í –ê–•–û-–ö–û–†–ê–°–ò–ö ===

üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(patterns)}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(set(patterns))}
- –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_analysis['total_duplicates']}

üîç –ê–ù–ê–õ–ò–ó –ö–û–†–û–¢–ö–ò–• –ü–ê–¢–¢–ï–†–ù–û–í (< 3 —Å–∏–º–≤–æ–ª–æ–≤):
- –í—Å–µ–≥–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö: {short_analysis['total_short']}
- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ: {dict(short_analysis['by_length'])}
- –ü—Ä–∏–º–µ—Ä—ã: {short_analysis['examples'][:10]}
- –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ: {len(short_analysis['suspicious'])} (–ø–µ—Ä–≤—ã–µ 5: {short_analysis['suspicious'][:5]})

üìè –ê–ù–ê–õ–ò–ó –î–õ–ò–ù–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í (> 200 —Å–∏–º–≤–æ–ª–æ–≤):
- –í—Å–µ–≥–æ –¥–ª–∏–Ω–Ω—ã—Ö: {long_analysis['total_long']}
- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {long_analysis['avg_length']:.2f}
- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {long_analysis['max_length']}
- –ü—Ä–∏–º–µ—Ä—ã: {long_analysis['examples'][:5]}

üîÑ –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í:
- –í—Å–µ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {duplicate_analysis['total_duplicates']}
- –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ: {duplicate_analysis['most_common'][:5]}
- –ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {list(duplicate_analysis['duplicate_examples'].items())[:5]}

üî§ –ê–ù–ê–õ–ò–ó –ü–†–û–ë–ï–õ–û–í:
- –ù–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å –ø—Ä–æ–±–µ–ª–∞: {len(whitespace_analysis['leading_spaces'])}
- –ó–∞–∫–∞–Ω—á–∏–≤–∞—é—â–∏—Ö—Å—è –ø—Ä–æ–±–µ–ª–æ–º: {len(whitespace_analysis['trailing_spaces'])}
- –° –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏: {len(whitespace_analysis['multiple_spaces'])}
- –° —Ç–∞–±—É–ª—è—Ü–∏—è–º–∏: {len(whitespace_analysis['tabs'])}
- –° –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫: {len(whitespace_analysis['newlines'])}

üìù –ê–ù–ê–õ–ò–ó –†–ï–ì–ò–°–¢–†–ê:
- –°–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä –≤ —Å–ª–æ–≤–∞—Ö: {len(case_analysis['mixed_case_words'])}
- –ù–µ–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è: {len(case_analysis['inconsistent_capitalization'])}
- –í—Å–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ: {len(case_analysis['all_uppercase'])}
- –í—Å–µ —Å—Ç—Ä–æ—á–Ω—ã–µ: {len(case_analysis['all_lowercase'])}

üîß –ê–ù–ê–õ–ò–ó –ö–û–î–ò–†–û–í–ö–ò:
- –û—à–∏–±–∫–∏ Unicode: {len(encoding_analysis['unicode_errors'])}
- –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã: {len(encoding_analysis['suspicious_chars'])}
- –ü—Ä–æ–±–ª–µ–º—ã roundtrip: {len(encoding_analysis['encoding_inconsistencies'])}

‚ö†Ô∏è –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê:
- –ü—É—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {len(quality_analysis['empty_patterns'])}
- –¢–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª—ã: {len(quality_analysis['whitespace_only'])}
- –û–¥–∏–Ω–æ—á–Ω—ã–µ –Ω–µ–±—É–∫–≤–µ–Ω–Ω—ã–µ: {len(quality_analysis['single_char_non_alpha'])}
- –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã: {len(quality_analysis['repeated_chars'])}
- –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ: {len(quality_analysis['suspicious_patterns'])}

üìã –î–ï–¢–ê–õ–¨–ù–´–ï –ü–†–ò–ú–ï–†–´ –ü–†–û–ë–õ–ï–ú:

–ü—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏:
{chr(10).join(whitespace_analysis['leading_spaces'][:5])}

–ü—Ä–æ–±–ª–µ–º—ã —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º:
{chr(10).join(case_analysis['mixed_case_words'][:5])}

–ü—Ä–æ–±–ª–µ–º—ã –∫–æ–¥–∏—Ä–æ–≤–∫–∏:
{chr(10).join(encoding_analysis['suspicious_chars'][:5])}

–ü—Ä–æ–±–ª–µ–º—ã –∫–∞—á–µ—Å—Ç–≤–∞:
{chr(10).join(quality_analysis['suspicious_patterns'][:5])}

=== –ö–û–ù–ï–¶ –î–ï–¢–ê–õ–¨–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê ===
"""
        
        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    analyzer = DetailedPatternAnalyzer()
    
    patterns_file = "src/ai_service/data/templates/aho_corasick_patterns.txt"
    
    if not os.path.exists(patterns_file):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {patterns_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    report = analyzer.generate_detailed_report(patterns_file)
    print(report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open("detailed_pattern_analysis_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: detailed_pattern_analysis_report.txt")


if __name__ == "__main__":
    main()
