#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Set, Any
import re
from collections import Counter

# Add path to src for imports
sys.path.append(str(Path(__file__).parent / "src"))

from ai_service.layers.variants.template_builder import TemplateBuilder
from ai_service.utils.logging_config import get_logger


class AhoCorasickPatternChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.template_builder = TemplateBuilder()
        
    def load_templates(self, templates_file: str) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(templates_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —à–∞–±–ª–æ–Ω–æ–≤ –∏–∑ {templates_file}: {e}")
            return []
    
    def load_patterns(self, patterns_file: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(patterns_file, 'r', encoding='utf-8') as f:
                return [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {patterns_file}: {e}")
            return []
    
    def analyze_pattern_quality(self, patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        analysis = {
            'total_patterns': len(patterns),
            'unique_patterns': len(set(patterns)),
            'duplicates': len(patterns) - len(set(patterns)),
            'empty_patterns': sum(1 for p in patterns if not p.strip()),
            'very_short_patterns': sum(1 for p in patterns if len(p.strip()) < 3),
            'very_long_patterns': sum(1 for p in patterns if len(p.strip()) > 200),
            'avg_length': sum(len(p) for p in patterns) / len(patterns) if patterns else 0,
            'language_distribution': self._analyze_language_distribution(patterns),
            'pattern_types': self._analyze_pattern_types(patterns),
            'special_characters': self._analyze_special_characters(patterns),
            'encoding_issues': self._analyze_encoding_issues(patterns)
        }
        return analysis
    
    def _analyze_language_distribution(self, patterns: List[str]) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö"""
        cyrillic_count = 0
        latin_count = 0
        mixed_count = 0
        other_count = 0
        
        cyrillic_pattern = re.compile(r'[–ê-–Ø–∞-—è–Å—ë–Ü—ñ–á—ó–Ñ—î“ê“ë]')
        latin_pattern = re.compile(r'[A-Za-z]')
        
        for pattern in patterns:
            has_cyrillic = bool(cyrillic_pattern.search(pattern))
            has_latin = bool(latin_pattern.search(pattern))
            
            if has_cyrillic and has_latin:
                mixed_count += 1
            elif has_cyrillic:
                cyrillic_count += 1
            elif has_latin:
                latin_count += 1
            else:
                other_count += 1
        
        return {
            'cyrillic': cyrillic_count,
            'latin': latin_count,
            'mixed': mixed_count,
            'other': other_count
        }
    
    def _analyze_pattern_types(self, patterns: List[str]) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        types = {
            'names': 0,
            'companies': 0,
            'numbers': 0,
            'dates': 0,
            'addresses': 0,
            'other': 0
        }
        
        name_pattern = re.compile(r'^[–ê-–Ø–∞-—èA-Za-z\s\.\-]+$')
        company_pattern = re.compile(r'(–æ–æ–æ|–∑–∞–æ|–æ–∞–æ|–ø–∞–æ|–∞–æ|–∏–ø|—á–ø|—Ñ–æ–ø|—Ç–æ–≤|–ø–ø|llc|ltd|inc|corp|co|gmbh|srl|spa|bv|nv|oy|ab|as|sa|ag)', re.IGNORECASE)
        number_pattern = re.compile(r'^\d+$')
        date_pattern = re.compile(r'\d{4}[\-\/\.]\d{1,2}[\-\/\.]\d{1,2}|\d{1,2}[\-\/\.]\d{1,2}[\-\/\.]\d{4}')
        
        for pattern in patterns:
            if number_pattern.match(pattern):
                types['numbers'] += 1
            elif date_pattern.search(pattern):
                types['dates'] += 1
            elif company_pattern.search(pattern):
                types['companies'] += 1
            elif name_pattern.match(pattern):
                types['names'] += 1
            else:
                types['other'] += 1
        
        return types
    
    def _analyze_special_characters(self, patterns: List[str]) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö"""
        special_chars = {
            'apostrophes': 0,
            'hyphens': 0,
            'dots': 0,
            'spaces': 0,
            'numbers': 0,
            'unicode_issues': 0
        }
        
        for pattern in patterns:
            if "'" in pattern or "'" in pattern:
                special_chars['apostrophes'] += 1
            if '-' in pattern:
                special_chars['hyphens'] += 1
            if '.' in pattern:
                special_chars['dots'] += 1
            if ' ' in pattern:
                special_chars['spaces'] += 1
            if re.search(r'\d', pattern):
                special_chars['numbers'] += 1
            if re.search(r'[^\x00-\x7F]', pattern) and not re.search(r'[–ê-–Ø–∞-—è–Å—ë–Ü—ñ–á—ó–Ñ—î“ê“ë]', pattern):
                special_chars['unicode_issues'] += 1
        
        return special_chars
    
    def _analyze_encoding_issues(self, patterns: List[str]) -> List[str]:
        """–ò—â–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π"""
        issues = []
        
        for i, pattern in enumerate(patterns[:1000]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 1000
            try:
                pattern.encode('utf-8')
            except UnicodeEncodeError:
                issues.append(f"Pattern {i}: encoding error")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            if re.search(r'[^\x00-\x7F–ê-–Ø–∞-—è–Å—ë–Ü—ñ–á—ó–Ñ—î“ê“ë\s\.\-\'\"\(\)]', pattern):
                issues.append(f"Pattern {i}: suspicious characters: {pattern[:50]}")
        
        return issues
    
    def check_template_coverage(self, templates: List[Dict[str, Any]], patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
        coverage = {
            'templates_with_patterns': 0,
            'templates_without_patterns': 0,
            'missing_search_patterns': 0,
            'missing_variants': 0,
            'missing_token_variants': 0,
            'coverage_issues': []
        }
        
        pattern_set = set(patterns)
        
        for template in templates:
            has_patterns = False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º search_patterns
            if 'search_patterns' in template and template['search_patterns']:
                for pattern in template['search_patterns']:
                    if isinstance(pattern, str) and pattern in pattern_set:
                        has_patterns = True
                    elif isinstance(pattern, str):
                        coverage['missing_search_patterns'] += 1
                        coverage['coverage_issues'].append(f"Missing search_pattern: {pattern}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º variants
            if 'variants' in template and template['variants']:
                for variant in template['variants']:
                    if isinstance(variant, str) and variant in pattern_set:
                        has_patterns = True
                    elif isinstance(variant, str):
                        coverage['missing_variants'] += 1
                        coverage['coverage_issues'].append(f"Missing variant: {variant}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º token_variants
            if 'token_variants' in template and template['token_variants']:
                for token, variants in template['token_variants'].items():
                    if isinstance(token, str) and token in pattern_set:
                        has_patterns = True
                    elif isinstance(token, str):
                        coverage['missing_token_variants'] += 1
                        coverage['coverage_issues'].append(f"Missing token: {token}")
                    
                    if isinstance(variants, list):
                        for variant in variants:
                            if isinstance(variant, str) and variant in pattern_set:
                                has_patterns = True
                            elif isinstance(variant, str):
                                coverage['missing_token_variants'] += 1
                                coverage['coverage_issues'].append(f"Missing token variant: {variant}")
            
            if has_patterns:
                coverage['templates_with_patterns'] += 1
            else:
                coverage['templates_without_patterns'] += 1
        
        return coverage
    
    def check_pattern_consistency(self, patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        consistency = {
            'case_consistency': self._check_case_consistency(patterns),
            'whitespace_consistency': self._check_whitespace_consistency(patterns),
            'duplicate_patterns': self._find_duplicate_patterns(patterns),
            'similar_patterns': self._find_similar_patterns(patterns),
            'inconsistent_encoding': self._check_encoding_consistency(patterns)
        }
        return consistency
    
    def _check_case_consistency(self, patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä–∞"""
        case_issues = []
        
        for i, pattern in enumerate(patterns):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä –≤ –æ–¥–Ω–æ–º —Å–ª–æ–≤–µ
            words = pattern.split()
            for word in words:
                if len(word) > 1 and word[0].islower() and any(c.isupper() for c in word[1:]):
                    case_issues.append(f"Pattern {i}: mixed case in word '{word}': {pattern}")
        
        return {
            'total_issues': len(case_issues),
            'issues': case_issues[:10]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
        }
    
    def _check_whitespace_consistency(self, patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–±–µ–ª–æ–≤"""
        whitespace_issues = []
        
        for i, pattern in enumerate(patterns):
            if pattern.startswith(' ') or pattern.endswith(' '):
                whitespace_issues.append(f"Pattern {i}: leading/trailing whitespace: '{pattern}'")
            if '  ' in pattern:  # –î–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
                whitespace_issues.append(f"Pattern {i}: multiple spaces: '{pattern}'")
        
        return {
            'total_issues': len(whitespace_issues),
            'issues': whitespace_issues[:10]
        }
    
    def _find_duplicate_patterns(self, patterns: List[str]) -> Dict[str, Any]:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        pattern_counts = Counter(patterns)
        duplicates = {pattern: count for pattern, count in pattern_counts.items() if count > 1}
        
        return {
            'total_duplicates': len(duplicates),
            'duplicate_examples': dict(list(duplicates.items())[:10])
        }
    
    def _find_similar_patterns(self, patterns: List[str]) -> Dict[str, Any]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        similar_groups = []
        pattern_set = set(patterns)
        
        for i, pattern1 in enumerate(patterns[:100]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 100
            similar = []
            for j, pattern2 in enumerate(patterns[i+1:i+101]):
                if self._are_similar(pattern1, pattern2):
                    similar.append(pattern2)
            
            if similar:
                similar_groups.append({
                    'base_pattern': pattern1,
                    'similar_patterns': similar[:5]  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                })
        
        return {
            'total_similar_groups': len(similar_groups),
            'similar_groups': similar_groups[:5]
        }
    
    def _are_similar(self, pattern1: str, pattern2: str, threshold: float = 0.8) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂–∏ –ª–∏ –¥–≤–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞"""
        if abs(len(pattern1) - len(pattern2)) > max(len(pattern1), len(pattern2)) * 0.3:
            return False
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—â–∏—Ö –ø–æ–¥—Å—Ç—Ä–æ–∫
        common_chars = sum(1 for c in pattern1 if c in pattern2)
        similarity = common_chars / max(len(pattern1), len(pattern2))
        
        return similarity >= threshold
    
    def _check_encoding_consistency(self, patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫–∏"""
        encoding_issues = []
        
        for i, pattern in enumerate(patterns):
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞—Ç–Ω–æ
                encoded = pattern.encode('utf-8')
                decoded = encoded.decode('utf-8')
                if decoded != pattern:
                    encoding_issues.append(f"Pattern {i}: encoding roundtrip failed")
            except UnicodeError as e:
                encoding_issues.append(f"Pattern {i}: Unicode error: {e}")
        
        return {
            'total_issues': len(encoding_issues),
            'issues': encoding_issues[:10]
        }
    
    def generate_report(self, templates_file: str, patterns_file: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        self.logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        templates = self.load_templates(templates_file)
        patterns = self.load_patterns(patterns_file)
        
        if not templates:
            return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —à–∞–±–ª–æ–Ω—ã"
        
        if not patterns:
            return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        quality_analysis = self.analyze_pattern_quality(patterns)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ —à–∞–±–ª–æ–Ω–æ–≤
        coverage_analysis = self.check_template_coverage(templates, patterns)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å
        consistency_analysis = self.check_pattern_consistency(patterns)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = f"""
=== –û–¢–ß–ï–¢ –û –ü–†–û–í–ï–†–ö–ï –ü–ê–¢–¢–ï–†–ù–û–í –ê–•–û-–ö–û–†–ê–°–ò–ö ===

üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å–µ–≥–æ —à–∞–±–ª–æ–Ω–æ–≤: {len(templates)}
- –í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {quality_analysis['total_patterns']}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {quality_analysis['unique_patterns']}
- –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {quality_analysis['duplicates']}
- –ü—É—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {quality_analysis['empty_patterns']}

üìè –î–õ–ò–ù–ê –ü–ê–¢–¢–ï–†–ù–û–í:
- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {quality_analysis['avg_length']:.2f} —Å–∏–º–≤–æ–ª–æ–≤
- –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö (< 3 —Å–∏–º–≤–æ–ª–æ–≤): {quality_analysis['very_short_patterns']}
- –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö (> 200 —Å–∏–º–≤–æ–ª–æ–≤): {quality_analysis['very_long_patterns']}

üåç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –Ø–ó–´–ö–ê–ú:
- –ö–∏—Ä–∏–ª–ª–∏—Ü–∞: {quality_analysis['language_distribution']['cyrillic']}
- –õ–∞—Ç–∏–Ω–∏—Ü–∞: {quality_analysis['language_distribution']['latin']}
- –°–º–µ—à–∞–Ω–Ω—ã–µ: {quality_analysis['language_distribution']['mixed']}
- –î—Ä—É–≥–∏–µ: {quality_analysis['language_distribution']['other']}

üè∑Ô∏è –¢–ò–ü–´ –ü–ê–¢–¢–ï–†–ù–û–í:
- –ò–º–µ–Ω–∞: {quality_analysis['pattern_types']['names']}
- –ö–æ–º–ø–∞–Ω–∏–∏: {quality_analysis['pattern_types']['companies']}
- –ß–∏—Å–ª–∞: {quality_analysis['pattern_types']['numbers']}
- –î–∞—Ç—ã: {quality_analysis['pattern_types']['dates']}
- –î—Ä—É–≥–∏–µ: {quality_analysis['pattern_types']['other']}

üîç –ü–û–ö–†–´–¢–ò–ï –®–ê–ë–õ–û–ù–û–í:
- –®–∞–±–ª–æ–Ω–æ–≤ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏: {coverage_analysis['templates_with_patterns']}
- –®–∞–±–ª–æ–Ω–æ–≤ –±–µ–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {coverage_analysis['templates_without_patterns']}
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö search_patterns: {coverage_analysis['missing_search_patterns']}
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö variants: {coverage_analysis['missing_variants']}
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö token_variants: {coverage_analysis['missing_token_variants']}

‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´ –ö–û–ù–°–ò–°–¢–ï–ù–¢–ù–û–°–¢–ò:
- –ü—Ä–æ–±–ª–µ–º—ã —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º: {consistency_analysis['case_consistency']['total_issues']}
- –ü—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏: {consistency_analysis['whitespace_consistency']['total_issues']}
- –î—É–±–ª–∏–∫–∞—Ç—ã: {consistency_analysis['duplicate_patterns']['total_duplicates']}
- –ü–æ—Ö–æ–∂–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {consistency_analysis['similar_patterns']['total_similar_groups']}
- –ü—Ä–æ–±–ª–µ–º—ã –∫–æ–¥–∏—Ä–æ–≤–∫–∏: {consistency_analysis['inconsistent_encoding']['total_issues']}

üìã –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        if quality_analysis['duplicates'] > 0:
            report += f"- –£–¥–∞–ª–∏—Ç—å {quality_analysis['duplicates']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
        
        if quality_analysis['empty_patterns'] > 0:
            report += f"- –£–¥–∞–ª–∏—Ç—å {quality_analysis['empty_patterns']} –ø—É—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
        
        if quality_analysis['very_short_patterns'] > 100:
            report += f"- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å {quality_analysis['very_short_patterns']} –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
        
        if coverage_analysis['templates_without_patterns'] > 0:
            report += f"- –î–æ–±–∞–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è {coverage_analysis['templates_without_patterns']} —à–∞–±–ª–æ–Ω–æ–≤\n"
        
        if consistency_analysis['case_consistency']['total_issues'] > 0:
            report += "- –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥–∏—Å—Ç—Ä –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö\n"
        
        if consistency_analysis['whitespace_consistency']['total_issues'] > 0:
            report += "- –û—á–∏—Å—Ç–∏—Ç—å –ø—Ä–æ–±–µ–ª—ã –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö\n"
        
        report += "\n=== –ö–û–ù–ï–¶ –û–¢–ß–ï–¢–ê ==="
        
        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    checker = AhoCorasickPatternChecker()
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    templates_file = "src/ai_service/data/templates/all_templates.json"
    patterns_file = "src/ai_service/data/templates/aho_corasick_patterns.txt"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(templates_file):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {templates_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    if not os.path.exists(patterns_file):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {patterns_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = checker.generate_report(templates_file, patterns_file)
    print(report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
    with open("aho_corasick_patterns_check_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("\n–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: aho_corasick_patterns_check_report.txt")


if __name__ == "__main__":
    main()
