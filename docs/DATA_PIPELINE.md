# Elasticsearch Data Pipeline - –ü–æ–ª–Ω—ã–π –¶–∏–∫–ª –ó–∞–≥—Ä—É–∑–∫–∏ –î–∞–Ω–Ω—ã—Ö

–ü–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ Elasticsearch –æ—Ç –∏—Å—Ö–æ–¥–Ω—ã—Ö JSON —Ñ–∞–π–ª–æ–≤ –¥–æ runtime –ø–æ–∏—Å–∫–∞.

## üéØ –û–±–∑–æ—Ä

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          DATA PIPELINE                                  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  JSON Files ‚Üí Data Loader ‚Üí AC Generator ‚Üí Vector Generator ‚Üí ES       ‚îÇ
‚îÇ     ‚Üì             ‚Üì              ‚Üì                ‚Üì              ‚Üì      ‚îÇ
‚îÇ  sanctions    normalize      patterns         embeddings      indices   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìä –≠—Ç–∞–ø—ã Pipeline

### –≠—Ç–∞–ø 1: –ò—Å—Ö–æ–¥–Ω—ã–µ –î–∞–Ω–Ω—ã–µ

**–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ:** `data/sanctions/*.json`

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON —Ñ–∞–π–ª–æ–≤:**
```json
{
  "persons": [
    {
      "id": "P001",
      "name": "–ò–≤–∞–Ω –ò–≤–∞–Ω–æ–≤–∏—á –ü–µ—Ç—Ä–æ–≤",
      "aliases": ["–ò.–ò. –ü–µ—Ç—Ä–æ–≤", "Ivan Petrov"],
      "date_of_birth": "1985-05-15",
      "country": "RU",
      "metadata": {
        "source": "OFAC",
        "list_type": "SDN"
      }
    }
  ],
  "organizations": [
    {
      "id": "O001",
      "name": "–û–û–û \"–†–æ–≥–∞ –∏ –ö–æ–ø—ã—Ç–∞\"",
      "aliases": ["–†–æ–≥–∞ –∏ –ö–æ–ø—ã—Ç–∞", "Roga i Kopyta LLC"],
      "country": "RU",
      "metadata": {
        "inn": "1234567890",
        "ogrn": "1234567890123"
      }
    }
  ]
}
```

**–§–∞–π–ª—ã:**
- `all_persons_full.json` - –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–µ—Ä—Å–æ–Ω
- `all_orgs_full.json` - –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
- –î—Ä—É–≥–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ø–∏—Å–∫–∏ (OFAC, EU, UK, etc.)

---

### –≠—Ç–∞–ø 2: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `src/ai_service/layers/search/sanctions_data_loader.py`

**–ü—Ä–æ—Ü–µ—Å—Å:**

```python
class SanctionsDataLoader:
    async def load_sanctions_data(self) -> SanctionsDataset:
        # 1. –ß—Ç–µ–Ω–∏–µ –≤—Å–µ—Ö JSON —Ñ–∞–π–ª–æ–≤ –∏–∑ data/sanctions/
        files = glob("data/sanctions/*.json")

        # 2. –ü–∞—Ä—Å–∏–Ω–≥ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
        for file in files:
            data = json.load(file)
            persons.extend(data.get("persons", []))
            orgs.extend(data.get("organizations", []))

        # 3. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ ID
        unique_persons = deduplicate_by_id(persons)
        unique_orgs = deduplicate_by_id(orgs)

        # 4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        cache_to_file("sanctions_cache.json", dataset)

        return SanctionsDataset(
            persons=unique_persons,
            organizations=unique_orgs
        )
```

**–í—ã—Ö–æ–¥:**
- `SanctionsDataset` - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç
- `data/sanctions/sanctions_cache.json` - –∫—ç—à (34 MB, TTL 24h)

**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**
- ~15,000 –ø–µ—Ä—Å–æ–Ω
- ~8,000 –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
- ~50,000 –∞–ª–∏–∞—Å–æ–≤

---

### –≠—Ç–∞–ø 3: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è AC Patterns

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `src/ai_service/layers/variants/templates/high_recall_ac_generator.py`

**Tier-Based Pattern System:**

#### Tier 0: Exact Matches (confidence = 1.0)
```python
def generate_tier0_patterns(name: str) -> List[str]:
    """
    –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –±–µ–∑ –≤–∞—Ä–∏–∞—Ü–∏–π
    """
    return [
        name.lower(),                    # "–∏–≤–∞–Ω –ø–µ—Ç—Ä–æ–≤"
        normalize_spaces(name.lower()),  # —É–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    ]
```

#### Tier 1: High Recall (confidence = 0.8-0.95)
```python
def generate_tier1_patterns(name: str) -> List[str]:
    """
    –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏–∏, —á–∞—Å—Ç–∏—á–Ω—ã–µ –∏–º–µ–Ω–∞, –∏–Ω–∏—Ü–∏–∞–ª—ã
    """
    patterns = []

    # –¢—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è
    patterns.append(transliterate(name, "ru", "en"))  # "Ivan Petrov"
    patterns.append(transliterate(name, "en", "ru"))  # –æ–±—Ä–∞—Ç–Ω–∞—è

    # –ß–∞—Å—Ç–∏—á–Ω—ã–µ –∏–º–µ–Ω–∞
    tokens = tokenize(name)
    patterns.extend(generate_partial_names(tokens))   # "–ü–µ—Ç—Ä–æ–≤ –ò.–ò."

    # –ò–Ω–∏—Ü–∏–∞–ª—ã
    patterns.extend(generate_initial_variants(tokens)) # "–ò. –ü–µ—Ç—Ä–æ–≤"

    # –ü–æ—Ä—è–¥–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
    patterns.extend(generate_token_permutations(tokens)) # "–ü–µ—Ç—Ä–æ–≤ –ò–≤–∞–Ω"

    return patterns
```

#### Tier 2: Medium Recall (confidence = 0.6-0.8)
```python
def generate_tier2_patterns(name: str) -> List[str]:
    """
    N-grams, —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    """
    patterns = []

    # Character n-grams (3-5 —Å–∏–º–≤–æ–ª–æ–≤)
    patterns.extend(generate_ngrams(name, min_n=3, max_n=5))

    # Phonetic variants
    patterns.extend(generate_phonetic_variants(name))  # –º–µ—Ç–∞—Ñ–æ–Ω—ã

    # –£–º–µ–Ω—å—à–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã
    patterns.extend(diminutive_variants(name))  # "–í–∞–Ω—è" ‚Üí "–ò–≤–∞–Ω"

    return patterns
```

#### Tier 3: Low Recall (confidence = 0.4-0.6)
```python
def generate_tier3_patterns(name: str) -> List[str]:
    """
    Fuzzy/typo-tolerant –≤–∞—Ä–∏–∞–Ω—Ç—ã
    """
    patterns = []

    # Typo simulation (1-2 –æ–ø–µ—á–∞—Ç–∫–∏)
    patterns.extend(generate_typo_variants(name, max_edits=2))

    # Keyboard layout errors
    patterns.extend(generate_layout_errors(name))  # ru/en —Ä–∞—Å–∫–ª–∞–¥–∫–∞

    # OCR errors (–≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–µ —Å–∏–º–≤–æ–ª—ã)
    patterns.extend(generate_ocr_errors(name))  # O‚Üî0, l‚Üî1

    return patterns
```

**–ü–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:**

```python
class HighRecallACGenerator:
    def generate_patterns_for_entity(
        self,
        entity: Dict,
        entity_type: str
    ) -> Dict[str, List[ACPattern]]:

        patterns = []

        # 1. –ò–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∏–º–µ–Ω–∏
        name = entity.get("name", "")
        patterns.extend(self._generate_all_tiers(name))

        # 2. –ò–∑ –∞–ª–∏–∞—Å–æ–≤
        for alias in entity.get("aliases", []):
            patterns.extend(self._generate_all_tiers(alias))

        # 3. –î–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ
        if entity_type == "org":
            # –£–±–∏—Ä–∞–µ–º legal forms
            core = extract_org_core(name)
            patterns.extend(self._generate_all_tiers(core))

            # –í–∞—Ä–∏–∞–Ω—Ç—ã —Å/–±–µ–∑ –∫–∞–≤—ã—á–µ–∫
            patterns.extend(generate_quote_variants(name))

        # 4. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ confidence
        unique_patterns = deduplicate_patterns(patterns)
        sorted_patterns = sort_by_confidence(unique_patterns)

        return {
            entity["id"]: sorted_patterns
        }
```

**–í—ã—Ö–æ–¥:**
- –°–ª–æ–≤–∞—Ä—å `{entity_id: [ACPattern(text, tier, confidence)]}`
- ~500,000-1,000,000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –≤—Å–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ –ø–∞–º—è—Ç–∏ (–Ω–µ –≤ —Ñ–∞–π–ª, —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π)

**–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:**
- Tier 0: ~23,000 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—Ç–æ—á–Ω—ã–µ)
- Tier 1: ~150,000 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (—Ç—Ä–∞–Ω—Å–ª–∏—Ç, —á–∞—Å—Ç–∏—á–Ω—ã–µ)
- Tier 2: ~400,000 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (n-grams, phonetic)
- Tier 3: ~500,000 –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (fuzzy, typos)

---

### –≠—Ç–∞–ø 4: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Vector Embeddings

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `scripts/generate_vectors.py`

**–ú–æ–¥–µ–ª—å:**
```python
model_name = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
# Multilingual model supporting ru/uk/en
# Output: 384-dimensional dense vectors
```

**–ü—Ä–æ—Ü–µ—Å—Å:**

```python
class VectorGenerator:
    def __init__(self):
        from sentence_transformers import SentenceTransformer
        self.model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
        )

    def generate_vector(self, text: str) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 384-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        """
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        normalized = normalize_text(text)

        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è embedding
        embedding = self.model.encode([normalized])

        # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ø–∏—Å–æ–∫
        vector = embedding[0].tolist()  # [float] * 384

        return vector

    def generate_for_corpus(
        self,
        dataset: SanctionsDataset
    ) -> Dict[str, List[float]]:

        vectors = {}

        # –î–ª—è –ø–µ—Ä—Å–æ–Ω
        for person in dataset.persons:
            name = person["name"]
            vector = self.generate_vector(name)
            vectors[person["id"]] = vector

        # –î–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        for org in dataset.organizations:
            name = org["name"]
            vector = self.generate_vector(name)
            vectors[org["id"]] = vector

        return vectors
```

**–í—ã—Ö–æ–¥:**
- `{entity_id: [float] * 384}`
- –†–∞–∑–º–µ—Ä –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞: 384 floats √ó 4 bytes = ~1.5 KB
- –†–∞–∑–º–µ—Ä –≤—Å–µ—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: 23,000 entities √ó 1.5 KB ‚âà 34 MB

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:**
- Batch encoding –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (batch_size=32)
- GPU acceleration –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø–∞–º—è—Ç–∏

---

### –≠—Ç–∞–ø 5: –°–æ–∑–¥–∞–Ω–∏–µ Elasticsearch –ò–Ω–¥–µ–∫—Å–æ–≤

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `scripts/elasticsearch_setup_and_warmup.py`

#### 5.1 Component Template

**–§–∞–π–ª:** `templates/elasticsearch/elasticsearch_component_template.json`

```json
{
  "template": {
    "settings": {
      "analysis": {
        "normalizer": {
          "case_insensitive_normalizer": {
            "type": "custom",
            "filter": ["lowercase", "asciifolding", "icu_folding"]
          }
        },
        "analyzer": {
          "icu_text_analyzer": {
            "tokenizer": "icu_tokenizer",
            "filter": ["icu_normalizer", "icu_folding", "lowercase"]
          },
          "char_ngram_analyzer": {
            "tokenizer": "keyword",
            "filter": ["lowercase", "asciifolding", {
              "type": "ngram",
              "min_gram": 3,
              "max_gram": 5
            }]
          },
          "shingle_analyzer": {
            "tokenizer": "icu_tokenizer",
            "filter": ["icu_normalizer", "icu_folding", "lowercase", {
              "type": "shingle",
              "min_shingle_size": 2,
              "max_shingle_size": 3
            }]
          }
        }
      }
    }
  }
}
```

**–°–æ–∑–¥–∞–Ω–∏–µ:**
```bash
curl -X PUT "localhost:9200/_component_template/watchlist_analyzers" \
  -H "Content-Type: application/json" \
  -d @elasticsearch_component_template.json
```

#### 5.2 Index Templates

**–î–ª—è –ø–µ—Ä—Å–æ–Ω:** `templates/elasticsearch/elasticsearch_persons_template.json`

```json
{
  "index_patterns": ["watchlist_persons_*"],
  "composed_of": ["watchlist_analyzers"],
  "template": {
    "mappings": {
      "properties": {
        "entity_id": {"type": "keyword"},
        "entity_type": {"type": "keyword"},
        "dob": {"type": "date", "format": "strict_date_optional_time||yyyy-MM-dd"},
        "country": {"type": "keyword"},

        "normalized_text": {
          "type": "keyword",
          "normalizer": "case_insensitive_normalizer"
        },
        "aliases": {
          "type": "keyword",
          "normalizer": "case_insensitive_normalizer"
        },

        "name_text": {
          "type": "text",
          "analyzer": "icu_text_analyzer"
        },
        "name_ngrams": {
          "type": "text",
          "analyzer": "char_ngram_analyzer"
        },
        "name_shingles": {
          "type": "text",
          "analyzer": "shingle_analyzer"
        },

        "name_vector": {
          "type": "dense_vector",
          "dims": 384,
          "index": true,
          "similarity": "cosine"
        },

        "ac_patterns": {
          "type": "text",
          "fields": {
            "keyword": {"type": "keyword"}
          }
        },

        "metadata": {"type": "object", "enabled": true}
      }
    }
  }
}
```

**–î–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π:** –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ, `watchlist_orgs_*` pattern

**–°–æ–∑–¥–∞–Ω–∏–µ:**
```bash
curl -X PUT "localhost:9200/_index_template/watchlist_persons_template" \
  -d @elasticsearch_persons_template.json

curl -X PUT "localhost:9200/_index_template/watchlist_orgs_template" \
  -d @elasticsearch_orgs_template.json
```

#### 5.3 –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤

```python
async def create_indices():
    timestamp = int(time.time())

    # –ù–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã —Å timestamp
    persons_index = f"watchlist_persons_v{timestamp}"
    orgs_index = f"watchlist_orgs_v{timestamp}"

    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ (—à–∞–±–ª–æ–Ω –ø—Ä–∏–º–µ–Ω–∏—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    await es_client.indices.create(index=persons_index)
    await es_client.indices.create(index=orgs_index)

    return persons_index, orgs_index
```

---

### –≠—Ç–∞–ø 6: Bulk Loading –≤ Elasticsearch

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `scripts/elasticsearch_setup_and_warmup.py`

**–ü—Ä–æ—Ü–µ—Å—Å:**

```python
async def bulk_load_data(
    dataset: SanctionsDataset,
    ac_patterns: Dict[str, List[ACPattern]],
    vectors: Dict[str, List[float]],
    persons_index: str,
    orgs_index: str
):

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ bulk actions –¥–ª—è –ø–µ—Ä—Å–æ–Ω
    person_actions = []
    for person in dataset.persons:
        person_id = person["id"]

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc = {
            "entity_id": person_id,
            "entity_type": "person",
            "dob": person.get("date_of_birth"),
            "country": person.get("country", "UNKNOWN"),

            # AC –ø–æ–ª—è
            "normalized_text": person["name"].lower(),
            "aliases": [a.lower() for a in person.get("aliases", [])],
            "name_text": person["name"],
            "name_ngrams": person["name"],
            "name_shingles": person["name"],

            # AC patterns
            "ac_patterns": [
                p.text for p in ac_patterns.get(person_id, [])
            ],

            # Vector
            "name_vector": vectors.get(person_id, [0.0] * 384),

            # Metadata
            "metadata": person.get("metadata", {})
        }

        # Bulk action
        person_actions.append({"index": {"_index": persons_index, "_id": person_id}})
        person_actions.append(doc)

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ bulk actions –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ)
    org_actions = []
    for org in dataset.organizations:
        # ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–µ—Ä—Å–æ–Ω–∞–º
        pass

    # 3. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ bulk –æ–ø–µ—Ä–∞—Ü–∏–π
    # Chunked bulk –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è timeout (chunk_size=500)
    await bulk_with_chunking(
        es_client,
        person_actions,
        chunk_size=500,
        max_retries=3
    )

    await bulk_with_chunking(
        es_client,
        org_actions,
        chunk_size=500,
        max_retries=3
    )

    # 4. Refresh –∏–Ω–¥–µ–∫—Å–æ–≤
    await es_client.indices.refresh(index=persons_index)
    await es_client.indices.refresh(index=orgs_index)

    # 5. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
    persons_count = await es_client.count(index=persons_index)
    orgs_count = await es_client.count(index=orgs_index)

    logger.info(f"Loaded {persons_count} persons, {orgs_count} orgs")
```

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
- Chunked bulk (500 docs per chunk)
- Retry logic (max 3 retries)
- Refresh –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
- –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ persons/orgs

**–í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏:**
- 15,000 –ø–µ—Ä—Å–æ–Ω: ~30 —Å–µ–∫—É–Ω–¥
- 8,000 –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: ~15 —Å–µ–∫—É–Ω–¥
- –û–±—â–µ–µ: ~45 —Å–µ–∫—É–Ω–¥

---

### –≠—Ç–∞–ø 7: Blue-Green Deployment —Å Aliases

**–¶–µ–ª—å:** Zero-downtime –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤

**–ü—Ä–æ—Ü–µ—Å—Å:**

```python
async def rollover_aliases(
    new_persons_index: str,
    new_orgs_index: str
):

    # 1. –ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –∞–ª–∏–∞—Å—ã
    old_persons_alias = await get_index_by_alias("watchlist_persons_current")
    old_orgs_alias = await get_index_by_alias("watchlist_orgs_current")

    # 2. –ê—Ç–æ–º–∞—Ä–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è
    actions = []

    # Persons
    if old_persons_alias:
        actions.append({
            "remove": {
                "index": old_persons_alias,
                "alias": "watchlist_persons_current"
            }
        })

    actions.append({
        "add": {
            "index": new_persons_index,
            "alias": "watchlist_persons_current"
        }
    })

    # Orgs
    if old_orgs_alias:
        actions.append({
            "remove": {
                "index": old_orgs_alias,
                "alias": "watchlist_orgs_current"
            }
        })

    actions.append({
        "add": {
            "index": new_orgs_index,
            "alias": "watchlist_orgs_current"
        }
    })

    # 3. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∞—Ç–æ–º–∞—Ä–Ω–æ–≥–æ update_aliases
    await es_client.indices.update_aliases(body={"actions": actions})

    # 4. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
    verify_aliases()

    logger.info(f"Rollover complete: {new_persons_index}, {new_orgs_index}")
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
watchlist_persons_current ‚Üí watchlist_persons_v1696234567
watchlist_orgs_current    ‚Üí watchlist_orgs_v1696234567
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ù—É–ª–µ–≤–æ–π downtime
- –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ (–∞—Ç–æ–º–∞—Ä–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è)
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å rollback (—Å—Ç–∞—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã –æ—Å—Ç–∞—é—Ç—Å—è)
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º

---

### –≠—Ç–∞–ø 8: Warmup –∏ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `scripts/elasticsearch_setup_and_warmup.py`

**–¶–µ–ª—å:** –ü—Ä–æ–≥—Ä–µ—Ç—å –∫—ç—à–∏ Elasticsearch –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
async def warmup_indices():

    # 1. kNN warmup - top 10 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    top_queries = [
        "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
        "–û–û–û –†–æ–≥–∞ –∏ –ö–æ–ø—ã—Ç–∞",
        "John Smith",
        "–ü–µ—Ç—Ä–æ –ü–æ—Ä–æ—à–µ–Ω–∫–æ",
        "–í–æ–ª–æ–¥–∏–º–∏—Ä –ó–µ–ª–µ–Ω—Å—å–∫–∏–π",
        # ... –≤—Å–µ–≥–æ 10 –∑–∞–ø—Ä–æ—Å–æ–≤
    ]

    for query in top_queries:
        # kNN search
        vector = generate_vector(query)
        await es_client.search(
            index="watchlist_persons_current",
            body={
                "knn": {
                    "field": "name_vector",
                    "query_vector": vector,
                    "k": 10,
                    "num_candidates": 100
                }
            }
        )

    # 2. AC warmup - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ–ª—è–º
    await es_client.search(
        index="watchlist_persons_current",
        body={
            "query": {
                "multi_match": {
                    "query": "–ü–µ—Ç—Ä–æ–≤",
                    "fields": ["normalized_text^2.0", "aliases^1.5"],
                    "fuzziness": 1
                }
            }
        }
    )

    # 3. Aggregations warmup
    await es_client.search(
        index="watchlist_persons_current",
        body={
            "size": 0,
            "aggs": {
                "countries": {"terms": {"field": "country"}}
            }
        }
    )

    logger.info("Warmup complete")
```

**–≠—Ñ—Ñ–µ–∫—Ç:**
- –ü–µ—Ä–≤—ã–π –ø–æ–∏—Å–∫: ~200ms
- –ü–æ—Å–ª–µ warmup: ~15-30ms
- kNN –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω –≤ –ø–∞–º—è—Ç—å
- Aggregation –∫—ç—à–∏ –ø—Ä–æ–≥—Ä–µ—Ç—ã

---

### –≠—Ç–∞–ø 9: Runtime Search

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç:** `src/ai_service/layers/search/hybrid_search_service.py`

#### 9.1 AC Search

```python
class ElasticsearchACAdapter:
    async def search(self, query: str, opts: SearchOpts) -> List[Candidate]:

        # Build multi-match query
        body = {
            "query": {
                "bool": {
                    "should": [
                        # Exact match (highest priority)
                        {
                            "term": {
                                "normalized_text.keyword": {
                                    "value": query.lower(),
                                    "boost": 2.0 * opts.ac_boost
                                }
                            }
                        },
                        # AC pattern match
                        {
                            "term": {
                                "ac_patterns.keyword": {
                                    "value": query,
                                    "boost": 3.0 * opts.ac_boost
                                }
                            }
                        },
                        # Multi-field fuzzy
                        {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "normalized_text^2.0",
                                    "aliases^1.5",
                                    "name_text^1.0"
                                ],
                                "fuzziness": opts.ac_fuzziness,  # 0-3
                                "type": "best_fields"
                            }
                        },
                        # N-gram match
                        {
                            "match": {
                                "name_ngrams": {
                                    "query": query,
                                    "boost": 0.8 * opts.ac_boost
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "size": opts.top_k,
            "min_score": opts.ac_min_score
        }

        response = await self.es_client.search(
            index="watchlist_persons_current",
            body=body
        )

        return parse_candidates(response)
```

**–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
- P50: 15-25ms
- P95: 30-50ms
- P99: 50-80ms

#### 9.2 Vector Search

```python
class ElasticsearchVectorAdapter:
    async def search(self, query: str, opts: SearchOpts) -> List[Candidate]:

        # Generate query vector
        query_vector = self.embedding_service.encode(query)

        # kNN search
        body = {
            "knn": {
                "field": "name_vector",
                "query_vector": query_vector,  # [float] * 384
                "k": opts.top_k,
                "num_candidates": opts.max_escalation_results,
                "similarity": opts.vector_min_score
            },
            "size": opts.top_k
        }

        response = await self.es_client.search(
            index="watchlist_persons_current",
            body=body
        )

        return parse_candidates(response)
```

**–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:**
- P50: 20-30ms
- P95: 40-60ms
- P99: 60-100ms

**HNSW –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:**
- `m`: 16 (connections per layer)
- `ef_construction`: 200
- `ef_search`: 100 (runtime)

#### 9.3 Hybrid Search —Å Escalation

```python
class HybridSearchService:
    async def find_candidates(
        self,
        normalized: NormalizationResult,
        text: str,
        opts: SearchOpts
    ) -> List[Candidate]:

        # 1. AC Search (–ø–µ—Ä–≤—ã–π —ç—Ç–∞–ø)
        ac_candidates = await self.ac_adapter.search(
            query=normalized.normalized,
            opts=opts
        )

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —ç—Å–∫–∞–ª–∞—Ü–∏—é
        best_ac_score = max(c.score for c in ac_candidates) if ac_candidates else 0.0

        should_escalate = (
            opts.enable_escalation and
            best_ac_score < opts.escalation_threshold  # default 0.6
        )

        if should_escalate:
            # 3. Vector Search (—ç—Å–∫–∞–ª–∞—Ü–∏—è)
            vector_candidates = await self.vector_adapter.search(
                query=text,
                opts=opts
            )

            # 4. Fusion (RRF - Reciprocal Rank Fusion)
            candidates = reciprocal_rank_fusion(
                ac_candidates,
                vector_candidates,
                ac_weight=0.7,      # 70% AC
                vector_weight=0.3   # 30% Vector
            )
        else:
            candidates = ac_candidates

        # 5. –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        candidates = deduplicate_by_id(candidates)

        # 6. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
        candidates = sorted(candidates, key=lambda c: c.score, reverse=True)
        candidates = candidates[:opts.top_k]

        return candidates
```

**RRF Formula:**
```python
def reciprocal_rank_fusion(
    ac_results: List[Candidate],
    vector_results: List[Candidate],
    ac_weight: float = 0.7,
    vector_weight: float = 0.3,
    k: int = 60  # RRF constant
) -> List[Candidate]:

    scores = defaultdict(float)

    # AC scores
    for rank, candidate in enumerate(ac_results, start=1):
        rrf_score = ac_weight / (k + rank)
        scores[candidate.id] += rrf_score

    # Vector scores
    for rank, candidate in enumerate(vector_results, start=1):
        rrf_score = vector_weight / (k + rank)
        scores[candidate.id] += rrf_score

    # Combine
    merged = [
        Candidate(id=id, score=score, ...)
        for id, score in scores.items()
    ]

    return sorted(merged, key=lambda c: c.score, reverse=True)
```

---

## üìä –ü–æ–ª–Ω–∞—è –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Pipeline

### –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö

| –≠—Ç–∞–ø | –†–∞–∑–º–µ—Ä | –í—Ä–µ–º—è |
|------|--------|-------|
| –ò—Å—Ö–æ–¥–Ω—ã–µ JSON —Ñ–∞–π–ª—ã | 50 MB | - |
| Sanctions cache | 34 MB | <1s |
| AC Patterns (–≤ –ø–∞–º—è—Ç–∏) | ~100 MB | 5-10s |
| Vectors (–≤ –ø–∞–º—è—Ç–∏) | ~34 MB | 60-90s (GPU) |
| ES –∏–Ω–¥–µ–∫—Å—ã (–Ω–∞ –¥–∏—Å–∫–µ) | ~200 MB | 45s |
| ES –∏–Ω–¥–µ–∫—Å—ã (–≤ RAM) | ~150 MB | - |

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏

| –û–ø–µ—Ä–∞—Ü–∏—è | –í—Ä–µ–º—è | Throughput |
|----------|-------|------------|
| JSON parsing | 2s | 25 MB/s |
| AC generation | 8s | ~125k patterns/s |
| Vector generation | 75s | ~300 vectors/s |
| Bulk loading | 45s | ~500 docs/s |
| Warmup | 5s | - |
| **TOTAL** | **~135s** | - |

### Runtime –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

| –ú–µ—Ç–æ–¥ | P50 | P95 | P99 |
|-------|-----|-----|-----|
| AC Search | 15ms | 30ms | 50ms |
| Vector Search | 25ms | 45ms | 70ms |
| Hybrid Search | 20ms | 40ms | 60ms |

### –ö–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞

| –ú–µ—Ç–æ–¥ | Precision | Recall | F1 |
|-------|-----------|--------|-----|
| AC (exact) | 0.95 | 0.75 | 0.84 |
| Vector (semantic) | 0.85 | 0.90 | 0.87 |
| Hybrid (fusion) | 0.92 | 0.88 | 0.90 |

---

## üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### Environment Variables

```bash
# Elasticsearch
ES_HOSTS=localhost:9200
ES_USERNAME=elastic
ES_PASSWORD=changeme
ES_VERIFY_SSL=false
ES_TIMEOUT=30

# Search Settings
ENABLE_AC_SEARCH=true
ENABLE_VECTOR_SEARCH=true
ENABLE_HYBRID_FUSION=true
ENABLE_ESCALATION=true

# AC Settings
AC_FUZZINESS=1
AC_MIN_SCORE=0.6
AC_BOOST=1.2

# Vector Settings
VECTOR_MIN_SCORE=0.5
VECTOR_BOOST=1.0
VECTOR_EF_SEARCH=100

# Hybrid Settings
ESCALATION_THRESHOLD=0.6
AC_WEIGHT=0.7
VECTOR_WEIGHT=0.3
```

### Feature Flags

```python
# src/ai_service/config/settings.py
ENABLE_FAISS_INDEX=true           # FAISS acceleration for vectors
ENABLE_AC_TIER0=true              # Exact AC patterns
ENABLE_AC_ES=true                 # AC patterns in Elasticsearch
ENABLE_VECTOR_ES=true             # Vectors in Elasticsearch
ENABLE_HYBRID_SEARCH=true         # Hybrid AC+Vector
```

---

## üöÄ Deployment Workflows

### Local Development

```bash
# 1. Start Elasticsearch
docker run -d -p 9200:9200 -e "discovery.type=single-node" \
  elasticsearch:8.11.0

# 2. Load sanctions data
python scripts/load_sanctions.py

# 3. Generate patterns & vectors
python scripts/generate_ac_patterns.py
python scripts/generate_vectors.py

# 4. Setup Elasticsearch
python scripts/elasticsearch_setup_and_warmup.py

# 5. Test search
python scripts/test_search.py "–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤"
```

### CI/CD Pipeline (GitHub Actions)

–°–º. `docs/SEARCH_DEPLOYMENT_PIPELINE.md`

```bash
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
git push origin main  # ‚Üí production
git push origin develop  # ‚Üí staging

# –†—É—á–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ
gh workflow run search-deployment.yml \
  --field environment=production
```

### Production Deployment

```bash
# Blue-Green deployment —Å zero downtime
python scripts/deploy_search_integration.py \
  --environment production \
  --artifacts-path artifacts/ \
  --es-url "$PROD_ES_URL" \
  --es-auth "$PROD_ES_AUTH" \
  --es-verify-ssl true
```

---

## üîç Monitoring & Debugging

### Health Check

```bash
# Elasticsearch cluster health
curl http://localhost:9200/_cluster/health

# Index statistics
curl http://localhost:9200/watchlist_persons_current/_stats

# Search quality
python scripts/evaluate_search.py --queries test_queries.json
```

### Debugging

```python
# Enable debug logging
export LOG_LEVEL=DEBUG

# Trace search execution
from ai_service.layers.search import HybridSearchService

service = HybridSearchService()
candidates = await service.find_candidates(
    normalized=norm_result,
    text="–ò–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤",
    opts=SearchOpts(trace_enabled=True)
)

print(candidates.trace)  # Detailed execution trace
```

### Metrics Collection

```python
# Get search metrics
metrics = search_service.get_metrics()

print(f"Total requests: {metrics.total_requests}")
print(f"AC requests: {metrics.ac_requests}")
print(f"Vector requests: {metrics.vector_requests}")
print(f"Hit rate: {metrics.hit_rate:.2%}")
print(f"P95 latency: {metrics.p95_latency_ms:.2f}ms")
```

---

## üìö Related Documentation

- **`SEARCH_DEPLOYMENT_PIPELINE.md`** - CI/CD –∏ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è
- **`ELASTICSEARCH_WATCHLIST_ADAPTER.md`** - Adapter –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
- **`templates/elasticsearch/README.md`** - –ò–Ω–¥–µ–∫—Å—ã –∏ mappings
- **`src/ai_service/layers/search/README.md`** - Search Layer API
- **`SEARCH_CONFIGURATION.md`** - –î–µ—Ç–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
- **`SEARCH_TROUBLESHOOTING.md`** - Troubleshooting guide