#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è —à–∞–±–ª–æ–Ω–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Set, Any
import re

# Add path to src for imports
sys.path.append(str(Path(__file__).parent / "src"))

from ai_service.utils.logging_config import get_logger


class TemplateCoverageChecker:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–∫—Ä—ã—Ç–∏—è —à–∞–±–ª–æ–Ω–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def load_templates(self, templates_file: str) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            with open(templates_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —à–∞–±–ª–æ–Ω–æ–≤ –∏–∑ {templates_file}: {e}")
            return []
    
    def load_patterns(self, patterns_file: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(patterns_file, 'r', encoding='utf-8') as f:
                return [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {patterns_file}: {e}")
            return []
    
    def extract_template_patterns(self, template: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —à–∞–±–ª–æ–Ω–∞"""
        patterns = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º search_patterns
        if 'search_patterns' in template and template['search_patterns']:
            for pattern in template['search_patterns']:
                if isinstance(pattern, str) and pattern.strip():
                    patterns.append(pattern.strip())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º variants
        if 'variants' in template and template['variants']:
            for variant in template['variants']:
                if isinstance(variant, str) and variant.strip():
                    patterns.append(variant.strip())
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º token_variants
        if 'token_variants' in template and template['token_variants']:
            for token, variants in template['token_variants'].items():
                if isinstance(token, str) and token.strip():
                    patterns.append(token.strip())
                
                if isinstance(variants, list):
                    for variant in variants:
                        if isinstance(variant, str) and variant.strip():
                            patterns.append(variant.strip())
        
        return patterns
    
    def check_coverage(self, templates: List[Dict[str, Any]], patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ —à–∞–±–ª–æ–Ω–æ–≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏"""
        pattern_set = set(patterns)
        
        coverage_stats = {
            'total_templates': len(templates),
            'templates_with_coverage': 0,
            'templates_without_coverage': 0,
            'total_template_patterns': 0,
            'covered_template_patterns': 0,
            'uncovered_template_patterns': 0,
            'coverage_percentage': 0.0,
            'templates_by_type': {},
            'uncovered_examples': [],
            'coverage_issues': []
        }
        
        for i, template in enumerate(templates):
            template_patterns = self.extract_template_patterns(template)
            coverage_stats['total_template_patterns'] += len(template_patterns)
            
            covered_patterns = []
            uncovered_patterns = []
            
            for pattern in template_patterns:
                if pattern in pattern_set:
                    covered_patterns.append(pattern)
                    coverage_stats['covered_template_patterns'] += 1
                else:
                    uncovered_patterns.append(pattern)
                    coverage_stats['uncovered_template_patterns'] += 1
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —à–∞–±–ª–æ–Ω–∞
            entity_type = template.get('entity_type', 'unknown')
            if entity_type not in coverage_stats['templates_by_type']:
                coverage_stats['templates_by_type'][entity_type] = {
                    'total': 0,
                    'with_coverage': 0,
                    'without_coverage': 0
                }
            
            coverage_stats['templates_by_type'][entity_type]['total'] += 1
            
            if covered_patterns:
                coverage_stats['templates_with_coverage'] += 1
                coverage_stats['templates_by_type'][entity_type]['with_coverage'] += 1
            else:
                coverage_stats['templates_without_coverage'] += 1
                coverage_stats['templates_by_type'][entity_type]['without_coverage'] += 1
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –Ω–µ–ø–æ–∫—Ä—ã—Ç—ã—Ö —à–∞–±–ª–æ–Ω–æ–≤
                if len(coverage_stats['uncovered_examples']) < 10:
                    coverage_stats['uncovered_examples'].append({
                        'template_index': i,
                        'entity_type': entity_type,
                        'original_text': template.get('original_text', '')[:100],
                        'uncovered_patterns': uncovered_patterns[:5]
                    })
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ø—Ä–æ–±–ª–µ–º—ã –ø–æ–∫—Ä—ã—Ç–∏—è
            if uncovered_patterns:
                coverage_stats['coverage_issues'].append({
                    'template_index': i,
                    'entity_type': entity_type,
                    'uncovered_count': len(uncovered_patterns),
                    'uncovered_patterns': uncovered_patterns[:3]
                })
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è
        if coverage_stats['total_template_patterns'] > 0:
            coverage_stats['coverage_percentage'] = (
                coverage_stats['covered_template_patterns'] / 
                coverage_stats['total_template_patterns'] * 100
            )
        
        return coverage_stats
    
    def analyze_missing_patterns(self, templates: List[Dict[str, Any]], patterns: List[str]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        pattern_set = set(patterns)
        missing_patterns = []
        
        for template in templates:
            template_patterns = self.extract_template_patterns(template)
            
            for pattern in template_patterns:
                if pattern not in pattern_set:
                    missing_patterns.append({
                        'pattern': pattern,
                        'entity_type': template.get('entity_type', 'unknown'),
                        'original_text': template.get('original_text', '')[:100]
                    })
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        missing_by_type = {}
        for missing in missing_patterns:
            entity_type = missing['entity_type']
            if entity_type not in missing_by_type:
                missing_by_type[entity_type] = []
            missing_by_type[entity_type].append(missing['pattern'])
        
        # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        pattern_counts = {}
        for missing in missing_patterns:
            pattern = missing['pattern']
            pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        most_common_missing = sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:20]
        
        return {
            'total_missing': len(missing_patterns),
            'unique_missing': len(set(m['pattern'] for m in missing_patterns)),
            'missing_by_type': {k: len(v) for k, v in missing_by_type.items()},
            'most_common_missing': most_common_missing,
            'missing_examples': missing_patterns[:20]
        }
    
    def check_pattern_quality(self, patterns: List[str]) -> Dict[str, Any]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        quality_stats = {
            'total_patterns': len(patterns),
            'unique_patterns': len(set(patterns)),
            'duplicates': len(patterns) - len(set(patterns)),
            'empty_patterns': sum(1 for p in patterns if not p.strip()),
            'very_short': sum(1 for p in patterns if len(p.strip()) < 3),
            'very_long': sum(1 for p in patterns if len(p.strip()) > 200),
            'avg_length': sum(len(p) for p in patterns) / len(patterns) if patterns else 0,
            'language_distribution': self._analyze_language_distribution(patterns),
            'pattern_types': self._analyze_pattern_types(patterns)
        }
        
        return quality_stats
    
    def _analyze_language_distribution(self, patterns: List[str]) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–æ–≤"""
        cyrillic_count = 0
        latin_count = 0
        mixed_count = 0
        other_count = 0
        
        cyrillic_pattern = re.compile(r'[–ê-–Ø–∞-—è–Å—ë–Ü—ñ–á—ó–Ñ—î“ê“ë]')
        latin_pattern = re.compile(r'[A-Za-z]')
        
        for pattern in patterns:
            has_cyrillic = bool(cyrillic_pattern.search(pattern))
            has_latin = bool(latin_pattern.search(pattern))
            
            if has_cyrillic and has_latin:
                mixed_count += 1
            elif has_cyrillic:
                cyrillic_count += 1
            elif has_latin:
                latin_count += 1
            else:
                other_count += 1
        
        return {
            'cyrillic': cyrillic_count,
            'latin': latin_count,
            'mixed': mixed_count,
            'other': other_count
        }
    
    def _analyze_pattern_types(self, patterns: List[str]) -> Dict[str, int]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        types = {
            'names': 0,
            'companies': 0,
            'numbers': 0,
            'dates': 0,
            'addresses': 0,
            'other': 0
        }
        
        name_pattern = re.compile(r'^[–ê-–Ø–∞-—èA-Za-z\s\.\-]+$')
        company_pattern = re.compile(r'(–æ–æ–æ|–∑–∞–æ|–æ–∞–æ|–ø–∞–æ|–∞–æ|–∏–ø|—á–ø|—Ñ–æ–ø|—Ç–æ–≤|–ø–ø|llc|ltd|inc|corp|co|gmbh|srl|spa|bv|nv|oy|ab|as|sa|ag)', re.IGNORECASE)
        number_pattern = re.compile(r'^\d+$')
        date_pattern = re.compile(r'\d{4}[\-\/\.]\d{1,2}[\-\/\.]\d{1,2}|\d{1,2}[\-\/\.]\d{1,2}[\-\/\.]\d{4}')
        
        for pattern in patterns:
            if number_pattern.match(pattern):
                types['numbers'] += 1
            elif date_pattern.search(pattern):
                types['dates'] += 1
            elif company_pattern.search(pattern):
                types['companies'] += 1
            elif name_pattern.match(pattern):
                types['names'] += 1
            else:
                types['other'] += 1
        
        return types
    
    def generate_coverage_report(self, templates_file: str, patterns_file: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –ø–æ–∫—Ä—ã—Ç–∏–∏"""
        self.logger.info("–ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ–∫—Ä—ã—Ç–∏—è —à–∞–±–ª–æ–Ω–æ–≤...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        templates = self.load_templates(templates_file)
        patterns = self.load_patterns(patterns_file)
        
        if not templates:
            return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —à–∞–±–ª–æ–Ω—ã"
        
        if not patterns:
            return "–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ
        coverage_stats = self.check_coverage(templates, patterns)
        missing_analysis = self.analyze_missing_patterns(templates, patterns)
        quality_stats = self.check_pattern_quality(patterns)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = f"""
=== –û–¢–ß–ï–¢ –û –ü–û–ö–†–´–¢–ò–ò –®–ê–ë–õ–û–ù–û–í –ü–ê–¢–¢–ï–†–ù–ê–ú–ò –ê–•–û-–ö–û–†–ê–°–ò–ö ===

üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–ö–†–´–¢–ò–Ø:
- –í—Å–µ–≥–æ —à–∞–±–ª–æ–Ω–æ–≤: {coverage_stats['total_templates']}
- –®–∞–±–ª–æ–Ω–æ–≤ —Å –ø–æ–∫—Ä—ã—Ç–∏–µ–º: {coverage_stats['templates_with_coverage']}
- –®–∞–±–ª–æ–Ω–æ–≤ –±–µ–∑ –ø–æ–∫—Ä—ã—Ç–∏—è: {coverage_stats['templates_without_coverage']}
- –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è —à–∞–±–ª–æ–Ω–æ–≤: {(coverage_stats['templates_with_coverage'] / coverage_stats['total_templates'] * 100):.2f}%

üîç –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–¢–¢–ï–†–ù–û–í:
- –í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —à–∞–±–ª–æ–Ω–∞—Ö: {coverage_stats['total_template_patterns']}
- –ü–æ–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {coverage_stats['covered_template_patterns']}
- –ù–µ–ø–æ–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {coverage_stats['uncovered_template_patterns']}
- –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {coverage_stats['coverage_percentage']:.2f}%

üìã –ü–û–ö–†–´–¢–ò–ï –ü–û –¢–ò–ü–ê–ú –®–ê–ë–õ–û–ù–û–í:
"""
        
        for entity_type, stats in coverage_stats['templates_by_type'].items():
            coverage_pct = (stats['with_coverage'] / stats['total'] * 100) if stats['total'] > 0 else 0
            report += f"- {entity_type}: {stats['with_coverage']}/{stats['total']} ({coverage_pct:.2f}%)\n"
        
        report += f"""
üîç –ê–ù–ê–õ–ò–ó –û–¢–°–£–¢–°–¢–í–£–Æ–©–ò–• –ü–ê–¢–¢–ï–†–ù–û–í:
- –í—Å–µ–≥–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {missing_analysis['total_missing']}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö: {missing_analysis['unique_missing']}
- –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–æ —Ç–∏–ø–∞–º: {missing_analysis['missing_by_type']}

üìà –ö–ê–ß–ï–°–¢–í–û –ü–ê–¢–¢–ï–†–ù–û–í:
- –í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {quality_stats['total_patterns']}
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {quality_stats['unique_patterns']}
- –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {quality_stats['duplicates']}
- –ü—É—Å—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {quality_stats['empty_patterns']}
- –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö: {quality_stats['very_short']}
- –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã—Ö: {quality_stats['very_long']}
- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {quality_stats['avg_length']:.2f} —Å–∏–º–≤–æ–ª–æ–≤

üåç –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –Ø–ó–´–ö–ê–ú:
- –ö–∏—Ä–∏–ª–ª–∏—Ü–∞: {quality_stats['language_distribution']['cyrillic']}
- –õ–∞—Ç–∏–Ω–∏—Ü–∞: {quality_stats['language_distribution']['latin']}
- –°–º–µ—à–∞–Ω–Ω—ã–µ: {quality_stats['language_distribution']['mixed']}
- –î—Ä—É–≥–∏–µ: {quality_stats['language_distribution']['other']}

üè∑Ô∏è –¢–ò–ü–´ –ü–ê–¢–¢–ï–†–ù–û–í:
- –ò–º–µ–Ω–∞: {quality_stats['pattern_types']['names']}
- –ö–æ–º–ø–∞–Ω–∏–∏: {quality_stats['pattern_types']['companies']}
- –ß–∏—Å–ª–∞: {quality_stats['pattern_types']['numbers']}
- –î–∞—Ç—ã: {quality_stats['pattern_types']['dates']}
- –î—Ä—É–≥–∏–µ: {quality_stats['pattern_types']['other']}

‚ö†Ô∏è –ü–†–ò–ú–ï–†–´ –ù–ï–ü–û–ö–†–´–¢–´–• –®–ê–ë–õ–û–ù–û–í:
"""
        
        for example in coverage_stats['uncovered_examples'][:5]:
            report += f"- {example['entity_type']}: {example['original_text']}...\n"
            report += f"  –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {example['uncovered_patterns']}\n"
        
        report += f"""
üìã –ù–ê–ò–ë–û–õ–ï–ï –ß–ê–°–¢–´–ï –û–¢–°–£–¢–°–¢–í–£–Æ–©–ò–ï –ü–ê–¢–¢–ï–†–ù–´:
"""
        
        for pattern, count in missing_analysis['most_common_missing'][:10]:
            report += f"- '{pattern}' (–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ {count} —à–∞–±–ª–æ–Ω–∞—Ö)\n"
        
        report += f"""
üìã –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
"""
        
        if coverage_stats['coverage_percentage'] < 80:
            report += f"- –ö—Ä–∏—Ç–∏—á–Ω–æ: –ø–æ–∫—Ä—ã—Ç–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ {coverage_stats['coverage_percentage']:.2f}%\n"
            report += "- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ñ–∞–π–ª –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫\n"
        
        if missing_analysis['unique_missing'] > 1000:
            report += f"- –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {missing_analysis['unique_missing']} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
            report += "- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
        
        if quality_stats['duplicates'] > 100:
            report += f"- –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {quality_stats['duplicates']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö\n"
            report += "- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n"
        
        if quality_stats['very_short'] > 100:
            report += f"- –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {quality_stats['very_short']} –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n"
            report += "- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–¥–∞–ª–∏—Ç—å –∏–ª–∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n"
        
        report += "\n=== –ö–û–ù–ï–¶ –û–¢–ß–ï–¢–ê ==="
        
        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    checker = TemplateCoverageChecker()
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    templates_file = "src/ai_service/data/templates/all_templates.json"
    patterns_file = "src/ai_service/data/templates/aho_corasick_patterns.txt"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(templates_file):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {templates_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    if not os.path.exists(patterns_file):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {patterns_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    report = checker.generate_coverage_report(templates_file, patterns_file)
    print(report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª
    with open("template_coverage_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print("\n–û—Ç—á–µ—Ç –æ –ø–æ–∫—Ä—ã—Ç–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: template_coverage_report.txt")


if __name__ == "__main__":
    main()
