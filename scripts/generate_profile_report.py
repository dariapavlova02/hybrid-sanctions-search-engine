#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ cProfile –∏ pyinstrument.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–æ–∑–¥–∞—ë—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π markdown-–æ—Ç—á—ë—Ç
—Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.
"""

import json
import pstats
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import statistics


def analyze_cprofile_results(profile_file: Path) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ cProfile."""
    if not profile_file.exists():
        return {}
    
    stats = pstats.Stats(str(profile_file))
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ñ—É–Ω–∫—Ü–∏—è–º
    functions = []
    for func, (cc, nc, tt, ct, callers) in stats.stats.items():
        filename, line, func_name = func
        functions.append({
            'filename': filename,
            'line': line,
            'function': func_name,
            'calls': cc,
            'ncalls': nc,
            'tottime': tt,
            'cumtime': ct,
            'callers': len(callers)
        })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ cumtime (–Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è)
    functions.sort(key=lambda x: x['cumtime'], reverse=True)
    
    return {
        'total_functions': len(functions),
        'top_functions': functions[:20],
        'total_cumtime': sum(f['cumtime'] for f in functions),
        'total_tottime': sum(f['tottime'] for f in functions),
        'total_calls': sum(f['calls'] for f in functions)
    }


def analyze_profiling_stats() -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∏–∑ profiling —É—Ç–∏–ª–∏—Ç."""
    try:
        from ai_service.utils.profiling import get_profiling_stats
        return get_profiling_stats()
    except ImportError:
        return {}


def identify_hotspots(cprofile_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """–í—ã—è–≤–ª–µ–Ω–∏–µ –≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ cProfile."""
    hotspots = []
    
    if not cprofile_data or 'top_functions' not in cprofile_data:
        return hotspots
    
    top_functions = cprofile_data['top_functions']
    total_cumtime = cprofile_data.get('total_cumtime', 1)
    
    for func in top_functions[:10]:  # TOP-10
        cumtime_pct = (func['cumtime'] / total_cumtime) * 100
        tottime_pct = (func['tottime'] / total_cumtime) * 100
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≥–æ—Ä—è—á–µ–π —Ç–æ—á–∫–∏
        hotspot_type = "unknown"
        recommendations = []
        
        if "token" in func['function'].lower() or "tokenize" in func['function'].lower():
            hotspot_type = "tokenization"
            recommendations = [
                "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏",
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (–ø—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å)",
                "–ò–∑–±–µ–≥–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤ split() –≤ —Ü–∏–∫–ª–∞—Ö"
            ]
        elif "morph" in func['function'].lower() or "parse" in func['function'].lower():
            hotspot_type = "morphology"
            recommendations = [
                "–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞",
                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LRU –∫—ç—à –¥–ª—è —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤",
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—ã–∑–æ–≤—ã pymorphy3"
            ]
        elif "role" in func['function'].lower() or "classify" in func['function'].lower():
            hotspot_type = "role_classification"
            recommendations = [
                "–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–æ–ª–µ–π",
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –≤ —Å–ª–æ–≤–∞—Ä—è—Ö (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å set –≤–º–µ—Å—Ç–æ list)",
                "–ü—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è"
            ]
        elif "normalize" in func['function'].lower():
            hotspot_type = "normalization"
            recommendations = [
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏",
                "–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏",
                "–ò–∑–±–µ–≥–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"
            ]
        elif "re." in func['filename'] or "regex" in func['filename']:
            hotspot_type = "regex"
            recommendations = [
                "–ü—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è",
                "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã regex",
                "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã"
            ]
        
        if cumtime_pct > 1.0:  # –ë–æ–ª—å—à–µ 1% –æ—Ç –æ–±—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
            hotspots.append({
                'function': func['function'],
                'filename': func['filename'],
                'line': func['line'],
                'type': hotspot_type,
                'cumtime': func['cumtime'],
                'cumtime_pct': cumtime_pct,
                'tottime': func['tottime'],
                'tottime_pct': tottime_pct,
                'calls': func['calls'],
                'avg_time_per_call': func['cumtime'] / max(func['calls'], 1),
                'recommendations': recommendations
            })
    
    return hotspots


def generate_optimization_recommendations(hotspots: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
    recommendations = []
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
    type_groups = {}
    for hotspot in hotspots:
        hotspot_type = hotspot['type']
        if hotspot_type not in type_groups:
            type_groups[hotspot_type] = []
        type_groups[hotspot_type].append(hotspot)
    
    for hotspot_type, hotspots_list in type_groups.items():
        if not hotspots_list:
            continue
            
        total_time = sum(h['cumtime'] for h in hotspots_list)
        total_calls = sum(h['calls'] for h in hotspots_list)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        all_recommendations = []
        for hotspot in hotspots_list:
            all_recommendations.extend(hotspot['recommendations'])
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_recommendations = list(set(all_recommendations))
        
        recommendations.append({
            'type': hotspot_type,
            'hotspots_count': len(hotspots_list),
            'total_time': total_time,
            'total_calls': total_calls,
            'avg_time_per_call': total_time / max(total_calls, 1),
            'recommendations': unique_recommendations,
            'priority': 'high' if total_time > 0.1 else 'medium' if total_time > 0.01 else 'low'
        })
    
    return recommendations


def create_performance_diagram(hotspots: List[Dict[str, Any]]) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Mermaid."""
    if not hotspots:
        return "```mermaid\ngraph TD\n    A[No performance data available] --> B[Run profiling first]\n```"
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
    type_times = {}
    for hotspot in hotspots:
        hotspot_type = hotspot['type']
        if hotspot_type not in type_times:
            type_times[hotspot_type] = 0
        type_times[hotspot_type] += hotspot['cumtime']
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏–∞–≥—Ä–∞–º–º—É
    diagram = "```mermaid\npie title Performance by Component\n"
    for hotspot_type, time in sorted(type_times.items(), key=lambda x: x[1], reverse=True):
        percentage = (time / sum(type_times.values())) * 100
        diagram += f'    "{hotspot_type}" : {percentage:.1f}\n'
    diagram += "```"
    
    return diagram


def generate_markdown_report(
    cprofile_data: Dict[str, Any],
    profiling_stats: Dict[str, Any],
    hotspots: List[Dict[str, Any]],
    recommendations: List[Dict[str, Any]]
) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è markdown-–æ—Ç—á—ë—Ç–∞."""
    
    report = """# –û—Ç—á—ë—Ç –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è AI Service Normalization Factory

## –û–±–∑–æ—Ä

–≠—Ç–æ—Ç –æ—Ç—á—ë—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ factory-–ø—É—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö.
–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —Å –ø–æ–º–æ—â—å—é cProfile –∏ pyinstrument –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç.

## –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è

- **–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ**: 20 —Ñ—Ä–∞–∑ (RU/UK/EN) –ø–æ 100 –∏—Ç–µ—Ä–∞—Ü–∏–π –∫–∞–∂–¥–∞—è
- **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã**: cProfile + pstats, pyinstrument, custom profiling utilities
- **–§–æ–∫—É—Å**: –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ —Ç–∏–ø–∞ "–Ü–≤–∞–Ω –ü–µ—Ç—Ä–æ–≤", "–û–û–û '–†–æ–º–∞—à–∫–∞' –ò–≤–∞–Ω –ò."

"""
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    if cprofile_data:
        report += f"""## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è

- **–í—Å–µ–≥–æ —Ñ—É–Ω–∫—Ü–∏–π**: {cprofile_data.get('total_functions', 0)}
- **–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è**: {cprofile_data.get('total_cumtime', 0):.4f} —Å–µ–∫—É–Ω–¥
- **–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è**: {cprofile_data.get('total_tottime', 0):.4f} —Å–µ–∫—É–Ω–¥
- **–í—Å–µ–≥–æ –≤—ã–∑–æ–≤–æ–≤**: {cprofile_data.get('total_calls', 0)}

"""
    
    # –î–∏–∞–≥—Ä–∞–º–º–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    report += "## –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º\n\n"
    report += create_performance_diagram(hotspots) + "\n\n"
    
    # TOP-10 –≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫
    if hotspots:
        report += "## TOP-10 –ì–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫\n\n"
        report += "| –§—É–Ω–∫—Ü–∏—è | –§–∞–π–ª | –í—Ä–µ–º—è (cum) | % | –í—ã–∑–æ–≤—ã | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ |\n"
        report += "|---------|------|-------------|---|--------|-------------|\n"
        
        for hotspot in hotspots[:10]:
            filename = Path(hotspot['filename']).name
            recommendations_str = "; ".join(hotspot['recommendations'][:2])  # –ü–µ—Ä–≤—ã–µ 2 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            report += f"| `{hotspot['function']}` | `{filename}` | {hotspot['cumtime']:.4f}s | {hotspot['cumtime_pct']:.1f}% | {hotspot['calls']} | {recommendations_str} |\n"
        
        report += "\n"
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    if recommendations:
        report += "## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n\n"
        
        for rec in recommendations:
            priority_emoji = "üî¥" if rec['priority'] == 'high' else "üü°" if rec['priority'] == 'medium' else "üü¢"
            report += f"### {priority_emoji} {rec['type'].title()} ({rec['priority'].upper()} priority)\n\n"
            report += f"- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫**: {rec['hotspots_count']}\n"
            report += f"- **–û–±—â–µ–µ –≤—Ä–µ–º—è**: {rec['total_time']:.4f}s\n"
            report += f"- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –≤—ã–∑–æ–≤**: {rec['avg_time_per_call']:.6f}s\n\n"
            
            report += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n"
            for i, recommendation in enumerate(rec['recommendations'], 1):
                report += f"{i}. {recommendation}\n"
            report += "\n"
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    report += """## –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

### 1. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞

**–ü—Ä–æ–±–ª–µ–º–∞**: –ü–æ–≤—Ç–æ—Ä–Ω—ã–µ –≤—ã–∑–æ–≤—ã pymorphy3 –¥–ª—è –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ —Ç–æ–∫–µ–Ω–æ–≤.

**–†–µ—à–µ–Ω–∏–µ**:
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def _morph_nominal_cached(token: str, language: str) -> str:
    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    pass
```

**–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç**: –°–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ 30-50% –¥–ª—è –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ç–æ–∫–µ–Ω–æ–≤.

### 2. –ü—Ä–µ–¥–∫–æ–º–ø–∏–ª—è—Ü–∏—è —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π

**–ü—Ä–æ–±–ª–µ–º–∞**: –ö–æ–º–ø–∏–ª—è—Ü–∏—è regex –≤ –∫–∞–∂–¥–æ–º –≤—ã–∑–æ–≤–µ.

**–†–µ—à–µ–Ω–∏–µ**:
```python
import re

# –ù–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª—è
TOKEN_SPLIT_PATTERN = re.compile(r"([,])")
INITIALS_PATTERN = re.compile(r"^((?:[A-Za-z–ê-–Ø–∞-—è–Ü–á–Ñ“ê—ñ—ó—î“ë]\.){2,})([A-Za-z–ê-–Ø–∞-—è–Ü–á–Ñ“ê—ñ—ó—î“ë].*)$")
```

**–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç**: –°–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ 10-20%.

### 3. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ —Å–ª–æ–≤–∞—Ä—è—Ö

**–ü—Ä–æ–±–ª–µ–º–∞**: –õ–∏–Ω–µ–π–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Å–ø–∏—Å–∫–∞—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤.

**–†–µ—à–µ–Ω–∏–µ**:
```python
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å set –≤–º–µ—Å—Ç–æ list –¥–ª—è O(1) –ø–æ–∏—Å–∫–∞
STOP_WORDS_SET = set(STOP_ALL)
```

**–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç**: –°–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ 5-15%.

### 4. –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–æ–ª–µ–π

**–ü—Ä–æ–±–ª–µ–º–∞**: –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤.

**–†–µ—à–µ–Ω–∏–µ**:
```python
from functools import lru_cache

@lru_cache(maxsize=500)
def _classify_token_cached(token: str, language: str) -> str:
    # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–æ–ª–∏
    pass
```

**–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç**: –°–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ 20-40%.

### 5. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π

**–ü—Ä–æ–±–ª–µ–º–∞**: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–∑–æ–≤—ã strip(), split() –≤ —Ü–∏–∫–ª–∞—Ö.

**–†–µ—à–µ–Ω–∏–µ**:
```python
# –ë–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
def process_tokens_batch(tokens: List[str]) -> List[str]:
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥
    pass
```

**–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç**: –°–Ω–∏–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ 10-25%.

## –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### –î–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ—Ä–∞–∑—É: ~X.XXX –º—Å
- –ü–∏–∫–æ–≤–æ–µ –≤—Ä–µ–º—è: ~X.XXX –º—Å
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: ~X MB

### –ü–æ—Å–ª–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–ø—Ä–æ–≥–Ω–æ–∑)
- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ñ—Ä–∞–∑—É: ~X.XXX –º—Å (-XX%)
- –ü–∏–∫–æ–≤–æ–µ –≤—Ä–µ–º—è: ~X.XXX –º—Å (-XX%)
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: ~X MB (-XX%)

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **–ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è**:
   - –í–Ω–µ–¥—Ä–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
   - –ü—Ä–µ–¥–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ –≤ —Å–ª–æ–≤–∞—Ä—è—Ö

2. **–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è** (1-2 –Ω–µ–¥–µ–ª–∏):
   - –î–æ–±–∞–≤–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–æ–ª–µ–π
   - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
   - –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ CI/CD

3. **–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è** (1 –º–µ—Å—è—Ü):
   - –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—É—Ç–∏ –Ω–∞ Cython
   - –í–Ω–µ–¥—Ä–∏—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
   - –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ production

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–ê–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö —É–∑–∫–∏—Ö –º–µ—Å—Ç –≤ factory-–ø—É—Ç–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–æ–ª–∂–Ω–∞ –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é
–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–ª—è—é—Ç—Å—è –æ—Å–Ω–æ–≤–Ω—ã–º
use case —Å–∏—Å—Ç–µ–º—ã.

"""
    
    return report


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–∞."""
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è...")
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    artifacts_dir = Path("artifacts")
    profile_file = artifacts_dir / "profile_stats.prof"
    
    # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    artifacts_dir.mkdir(exist_ok=True)
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    print("–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ cProfile...")
    cprofile_data = analyze_cprofile_results(profile_file)
    
    print("–ê–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è...")
    profiling_stats = analyze_profiling_stats()
    
    print("–í—ã—è–≤–ª–µ–Ω–∏–µ –≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫...")
    hotspots = identify_hotspots(cprofile_data)
    
    print("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
    recommendations = generate_optimization_recommendations(hotspots)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    print("–°–æ–∑–¥–∞–Ω–∏–µ markdown-–æ—Ç—á—ë—Ç–∞...")
    report = generate_markdown_report(cprofile_data, profiling_stats, hotspots, recommendations)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞
    report_file = artifacts_dir / "profile_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"–û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_file}")
    
    # –í—ã–≤–æ–¥ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏
    print("\n" + "="*60)
    print("–ö–†–ê–¢–ö–ê–Ø –°–í–û–î–ö–ê")
    print("="*60)
    
    if hotspots:
        print(f"–ù–∞–π–¥–µ–Ω–æ –≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–µ–∫: {len(hotspots)}")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤ –≥–æ—Ä—è—á–∏—Ö —Ç–æ—á–∫–∞—Ö: {sum(h['cumtime'] for h in hotspots):.4f}s")
        
        print("\nTOP-3 –≥–æ—Ä—è—á–∏–µ —Ç–æ—á–∫–∏:")
        for i, hotspot in enumerate(hotspots[:3], 1):
            print(f"{i}. {hotspot['function']} - {hotspot['cumtime']:.4f}s ({hotspot['cumtime_pct']:.1f}%)")
    
    if recommendations:
        print(f"\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {len(recommendations)}")
        high_priority = [r for r in recommendations if r['priority'] == 'high']
        if high_priority:
            print(f"–í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {len(high_priority)}")
    
    print(f"\n–ü–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç: {report_file}")


if __name__ == "__main__":
    main()
