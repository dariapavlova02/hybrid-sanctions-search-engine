"""
Signals Service - —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π.

–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
- –í—ã–¥–µ–ª–µ–Ω–∏–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º –∏ –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
- –ü–∞—Ä—Å–∏–Ω–≥ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ (–ò–ù–ù/–Ñ–î–†–ü–û–£/–û–ì–†–ù/VAT)
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç —Ä–æ–∂–¥–µ–Ω–∏—è
- –°–∫–æ—Ä–∏–Ω–≥ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ evidence
- –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –ø–æ–ª–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π

–ù–ï –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ (—ç—Ç–æ –¥–µ–ª–∞–µ—Ç NormalizationService)
- –†–µ—à–µ–Ω–∏–µ "–ø—Ä–æ–ø—É—Å–∫–∞—Ç—å/–Ω–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å" (—ç—Ç–æ –¥–µ–ª–∞–µ—Ç SmartFilter)
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—ç—Ç–æ –¥–µ–ª–∞–µ—Ç EmbeddingService —Å EmbeddingPreprocessor)
"""

import asyncio
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from ...utils.logging_config import get_logger
from .extractors import (
    BirthdateExtractor,
    IdentifierExtractor,
    OrganizationExtractor,
    PersonExtractor,
)


# Confidence scoring constants
class ConfidenceScoring:
    """Constants for confidence scoring algorithm."""

    BASE_CONFIDENCE = 0.5

    # Person scoring bonuses
    BIRTHDATE_BONUS = 0.15
    VALID_ID_BONUS = 0.2
    INVALID_ID_BONUS = 0.1
    NAME_PATTERN_BONUS = 0.1

    # Organization scoring bonuses
    LEGAL_FORM_BONUS = 0.3
    QUOTED_CORE_BONUS = 0.2
    NORM_MATCH_BONUS = 0.2
    ADJACENT_NAME_BONUS = 0.1
    ORG_CORE_BONUS = 0.1
    CONTEXT_BONUS = 0.15  # Bonus for contextual organization signals
    ADDRESS_BONUS = 0.1   # Bonus for address-like context

    # Multi-evidence bonuses
    PERSON_MAX_MULTI_BONUS = 0.2
    ORG_MAX_MULTI_BONUS = 0.25
    MULTI_EVIDENCE_INCREMENT = 0.05

    MAX_CONFIDENCE = 1.0


@dataclass
class PersonSignal:
    """–°–∏–≥–Ω–∞–ª –æ –ø–µ—Ä—Å–æ–Ω–µ"""

    core: List[str]  # –¢–æ–∫–µ–Ω—ã –∏–º–µ–Ω–∏ –∏–∑ normalization
    full_name: str
    dob: Optional[str] = None  # ISO YYYY-MM-DD
    dob_raw: Optional[str] = None
    ids: List[Dict[str, Any]] = field(default_factory=list)
    confidence: float = 0.0
    evidence: List[str] = field(default_factory=list)


@dataclass
class OrganizationSignal:
    """–°–∏–≥–Ω–∞–ª –æ–± –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""

    core: str  # –Ø–¥—Ä–æ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ normalization
    legal_form: Optional[str] = None
    full: Optional[str] = None  # –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    ids: List[Dict[str, Any]] = field(default_factory=list)
    confidence: float = 0.0
    evidence: List[str] = field(default_factory=list)


@dataclass
class ExtraSignal:
    """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã"""

    type: str
    value: str
    raw: str
    confidence: float = 0.0


class SignalsService:
    """
    –°–µ—Ä–≤–∏—Å —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π.

    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç + —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏,
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–µ—Ä—Å–æ–Ω—ã –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
    —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (—é—Ä. —Ñ–æ—Ä–º—ã, ID, –¥–∞—Ç—ã —Ä–æ–∂–¥–µ–Ω–∏—è).
    """

    def __init__(self):
        self.logger = get_logger(__name__)

        # Initialize specialized extractors
        self.person_extractor = PersonExtractor()
        self.organization_extractor = OrganizationExtractor()
        self.identifier_extractor = IdentifierExtractor()
        self.birthdate_extractor = BirthdateExtractor()

        # Text storage for proximity matching
        self._current_text = ""

        self.logger.info("SignalsService initialized with extractors")

    def extract(
        self,
        text: str,
        normalization_result: Optional[Dict[str, Any]] = None,
        language: str = "uk",
    ) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            normalization_result: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã NormalizationService (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π)
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞

        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏:
            - persons: List[PersonSignal]
            - organizations: List[OrganizationSignal]
            - extras: Dict —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª—ã
        """
        if not text or not text.strip():
            return self._empty_result()

        # Debug logging for input validation
        self.logger.debug(f"SIGNALS DEBUG: extract_signals called with text='{text}', language='{language}'")
        self.logger.debug(f"SIGNALS DEBUG: normalization_result type: {type(normalization_result)}")

        # Convert NormalizationResult object to dict if needed
        if normalization_result and hasattr(normalization_result, 'to_dict'):
            self.logger.debug(f"SIGNALS DEBUG: Converting NormalizationResult to dict")
            normalization_result = normalization_result.to_dict()
        elif normalization_result and not isinstance(normalization_result, dict):
            # If it's already a dict, use as is, otherwise try to convert
            try:
                normalization_result = dict(normalization_result)
            except (TypeError, ValueError):
                self.logger.warning(f"Cannot convert normalization_result to dict: {type(normalization_result)}")
                normalization_result = None
        elif normalization_result and not isinstance(normalization_result, dict):
            # If it's not a dict and doesn't have to_dict, convert it
            try:
                normalization_result = dict(normalization_result)
            except (TypeError, ValueError):
                self.logger.warning(f"Cannot convert normalization_result to dict: {type(normalization_result)}")
                normalization_result = None

        # –õ–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –≤ –∏—Ç–æ–≥–µ –ø–µ—Ä–µ–¥–∞–Ω–æ –≤ _get_entity_cores
        if normalization_result:
            self.logger.info(f"[CHECK] SIGNALS DEBUG: normalization_result keys: {list(normalization_result.keys())}")
            self.logger.info(f"[CHECK] SIGNALS DEBUG: persons_core present: {'persons_core' in normalization_result}")
            print(f"[CHECK] SIGNALS DEBUG: normalization_result keys: {list(normalization_result.keys())}")
            print(f"[CHECK] SIGNALS DEBUG: persons_core present: {'persons_core' in normalization_result}")
        else:
            self.logger.warning(f"[CHECK] SIGNALS DEBUG: normalization_result is None/empty!")
            print(f"[CHECK] SIGNALS DEBUG: normalization_result is None/empty!")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è proximity matching
        self._current_text = text

        try:
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
            persons_core, organizations_core = self._get_entity_cores(
                text, normalization_result, language
            )

            # 2. –°–æ–∑–¥–∞–µ–º –ø–µ—Ä—Å–æ–Ω—ã –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤
            persons = self._create_person_signals(persons_core)

            # 3. –°–æ–∑–¥–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ–º persons_core, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –§–ò–û)
            organizations = self._create_organization_signals(text, organizations_core, persons_core)

            # 4. –û–±–æ–≥–∞—â–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏
            self._enrich_with_identifiers(text, persons, organizations, normalization_result)

            # 5. –û–±–æ–≥–∞—â–∞–µ–º –ø–µ—Ä—Å–æ–Ω—ã –¥–∞—Ç–∞–º–∏ —Ä–æ–∂–¥–µ–Ω–∏—è
            self._enrich_with_birthdates(text, persons)

            # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥ —Å—É—â–Ω–æ—Å—Ç–µ–π
            self._score_entities(persons, organizations)

            # 7. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = self._build_result(persons, organizations)

            self.logger.debug(
                f"Extracted {len(persons)} persons, {len(organizations)} organizations"
            )

            return result

        except Exception as e:
            self.logger.error(f"Error extracting signals: {e}")
            return self._empty_result()

    def _get_entity_cores(
        self,
        text: str,
        normalization_result: Optional[Dict[str, Any]],
        language: str,
    ) -> tuple[List[List[str]], List[str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–µ—Ä—Å–æ–Ω –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π."""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä—Å–æ–Ω—ã - –ü–†–ò–û–†–ò–¢–ï–¢ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        if normalization_result and "persons_core" in normalization_result:
            persons_core = normalization_result["persons_core"]
            self.logger.info(f"üü¢ SIGNALS FIX: Using normalized persons_core: {persons_core}")
            print(f"üü¢ SIGNALS FIX: Using normalized persons_core: {persons_core}")

            # –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ø—Ä–æ–≤–µ—Ä–∏–º —á—Ç–æ –≤ persons_core –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            filtered_persons_core = []
            for person_tokens in persons_core:
                filtered_tokens = []
                for token in person_tokens:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω payment/stopword
                    if self._is_valid_person_token(token, language):
                        filtered_tokens.append(token)
                    else:
                        self.logger.warning(f"üî¥ FILTERING OUT invalid person token: '{token}'")
                        print(f"üî¥ FILTERING OUT invalid person token: '{token}'")

                if filtered_tokens:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                    filtered_persons_core.append(filtered_tokens)

            persons_core = filtered_persons_core
            self.logger.info(f"üü¢ AFTER FILTERING: persons_core: {persons_core}")
            print(f"üü¢ AFTER FILTERING: persons_core: {persons_core}")

            # FALLBACK: –µ—Å–ª–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å, –ø–æ–ø—Ä–æ–±—É–µ–º PersonExtractor
            if not persons_core:
                self.logger.warning(f"[WARN] EMPTY AFTER FILTERING: Trying PersonExtractor as fallback")
                print(f"[WARN] EMPTY AFTER FILTERING: Trying PersonExtractor as fallback")
                fallback_persons = self.person_extractor.extract(text, language)

                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–∞–∫—É—é –∂–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∫ fallback —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
                for person_tokens in fallback_persons:
                    filtered_tokens = []
                    for token in person_tokens:
                        if self._is_valid_person_token(token, language):
                            filtered_tokens.append(token)
                    if filtered_tokens:
                        persons_core.append(filtered_tokens)

                self.logger.info(f"[WARN] FALLBACK RESULT: persons_core: {persons_core}")
                print(f"[WARN] FALLBACK RESULT: persons_core: {persons_core}")
        else:
            # FALLBACK: –∏—Å–ø–æ–ª—å–∑—É–µ–º PersonExtractor —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            self.logger.warning(f"üî¥ SIGNALS FALLBACK: No persons_core in normalization_result, falling back to PersonExtractor. normalization_result keys: {list(normalization_result.keys()) if normalization_result else 'None'}")
            print(f"üî¥ SIGNALS FALLBACK: No persons_core in normalization_result, falling back to PersonExtractor")
            persons_core = self.person_extractor.extract(text, language)

        # Ensure persons_core is not None
        if persons_core is None:
            persons_core = []

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ - –ü–†–ò–û–†–ò–¢–ï–¢ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        if normalization_result and "organizations_core" in normalization_result:
            organizations_core = normalization_result["organizations_core"]
            self.logger.debug(f"Using normalized organizations_core: {organizations_core}")
        else:
            # FALLBACK: –∏—Å–ø–æ–ª—å–∑—É–µ–º OrganizationExtractor —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            self.logger.warning("No organizations_core in normalization_result, falling back to OrganizationExtractor")
            organizations_core = self.organization_extractor.extract(text, language)

        # Ensure organizations_core is not None
        if organizations_core is None:
            organizations_core = []

        return persons_core, organizations_core

    def _is_valid_person_token(self, token: str, language: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–º –¥–ª—è –ø–µ—Ä—Å–æ–Ω—ã (–Ω–µ stopword/payment context)."""
        if not token or len(token) < 2:
            return False

        token_lower = token.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º payment context —Å–ª–æ–≤–∞
        payment_words = {
            "—Å–ø–ª–∞—Ç–∞", "–ø–ª–∞—Ç–µ–∂", "–æ–ø–ª–∞—Ç–∞", "–ø–ª–∞—Ç—ñ–∂", "–¥–æ–≥–æ–≤–æ—Ä", "–¥–æ–≥–æ–≤–æ—Ä—É", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç",
            "—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ", "—É–≥–æ–¥–∞", "–∞–±–æ–Ω", "–ø–ª–∞—Ç–∞", "–ø–ª–∞—Ç–∏", "—É—Å–ª—É–≥", "–ø–æ—Å–ª—É–≥",
        }
        if token_lower in payment_words:
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º stopwords
        stopwords = {
            "–ø–æ", "–æ—Ç", "–¥–ª—è", "–≤", "–Ω–∞", "—Å", "–∫", "—É", "–∑", "–≤—ñ–¥", "—Ç–∞", "—ñ",
            "–∏", "–∞", "–Ω–æ", "–∏–ª–∏", "–ª–∏–±–æ", "—á–∏", "–∞–±–æ", "‚Ññ", "–Ω–æ–º–µ—Ä",
        }
        if token_lower in stopwords:
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—ã –∏ –Ω–æ–º–µ—Ä–∞
        if re.match(r'^\d+[\.\-/]\d+[\.\-/]\d+', token) or re.match(r'^\d{8,}', token):
            return False

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —ç—Ç–æ –ª–∏ –∏–º—è?
        if self._looks_like_person_name(token, language):
            return True

        # –ï—Å–ª–∏ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –∫–∞–∫ –∏–º—è, –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        return len(token) >= 3 and token[0].isupper() and token[1:].islower()

    def _looks_like_person_name(self, token: str, language: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ö–æ–∂ –ª–∏ —Ç–æ–∫–µ–Ω –Ω–∞ –∏–º—è —á–µ–ª–æ–≤–µ–∫–∞."""
        if not token:
            return False

        token_lower = token.lower()

        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —É–∫—Ä–∞–∏–Ω—Å–∫–∏–µ –∏ —Ä—É—Å—Å–∫–∏–µ –∏–º–µ–Ω–∞
        common_names = {
            # –ñ–µ–Ω—Å–∫–∏–µ –∏–º–µ–Ω–∞
            "–∫–∞—Ç–µ—Ä–∏–Ω–∞", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–∞", "–∞–Ω–Ω–∞", "–º–∞—Ä—ñ—è", "–º–∞—Ä–∏—è", "–æ–ª—å–≥–∞", "–Ω–∞—Ç–∞–ª—ñ—è", "–Ω–∞—Ç–∞–ª—å—è",
            "—é–ª—ñ—è", "—é–ª–∏—è", "—ñ—Ä–∏–Ω–∞", "–∏—Ä–∏–Ω–∞", "—Å–≤—ñ—Ç–ª–∞–Ω–∞", "—Å–≤–µ—Ç–ª–∞–Ω–∞", "—Ç–µ—Ç—è–Ω–∞", "—Ç–∞—Ç—å—è–Ω–∞",
            "–ª—é–¥–º–∏–ª–∞", "–≤–∞–ª–µ—Ä–∏—è", "–≤–∞–ª–µ–Ω—Ç–∏–Ω–∞", "–æ–∫—Å–∞–Ω–∞", "–ª–∞—Ä–∏—Å–∞", "–≤—ñ—Ä–∞", "–≤–µ—Ä–∞", "–Ω–∞–¥—ñ—è", "–Ω–∞–¥–µ–∂–¥–∞",
            # –ú—É–∂—Å–∫–∏–µ –∏–º–µ–Ω–∞
            "–æ–ª–µ–∫—Å–∞–Ω–¥—Ä", "–∞–ª–µ–∫—Å–∞–Ω–¥—Ä", "–¥–º–∏—Ç—Ä–æ", "–¥–º–∏—Ç—Ä–∏–π", "–∞–Ω–¥—Ä—ñ–π", "–∞–Ω–¥—Ä–µ–π", "—Å–µ—Ä–≥—ñ–π", "—Å–µ—Ä–≥–µ–π",
            "–≤–æ–ª–æ–¥–∏–º–∏—Ä", "–≤–ª–∞–¥–∏–º–∏—Ä", "—ñ–≤–∞–Ω", "–∏–≤–∞–Ω", "–ø–µ—Ç—Ä–æ", "–ø–µ—Ç—Ä", "–º–∞–∫—Å–∏–º", "–º–∏—Ö–∞–π–ª–æ", "–º–∏—Ö–∞–∏–ª",
            "–∞—Ä—Ç–µ–º", "—Ä–æ–º–∞–Ω", "–≤—ñ—Ç–∞–ª—ñ–π", "–≤–∏—Ç–∞–ª–∏–π", "–∏–≥–æ—Ä—å", "—ñ–≥–æ—Ä", "—é—Ä—ñ–π", "—é—Ä–∏–π", "—î–≤–≥–µ–Ω", "–µ–≤–≥–µ–Ω–∏–π",
        }

        if token_lower in common_names:
            return True

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–∏–ø–∏—á–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è –∏–º–µ–Ω
        if language in ["uk", "ru"]:
            # –ñ–µ–Ω—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–≤–∫–ª—é—á–∞—è -–æ–≤–∞ –¥–ª—è —Ñ–∞–º–∏–ª–∏–π)
            if token_lower.endswith(("–Ω–∞", "–∏–Ω–∞", "–æ–≤–∞", "–µ–≤–∞", "—ñ—è", "—è", "–∫–∞", "–ª–∞")):
                return len(token) >= 4
            # –ú—É–∂—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–≤–∫–ª—é—á–∞—è -–æ–≤/-–µ–≤ –¥–ª—è —Ñ–∞–º–∏–ª–∏–π)
            if token_lower.endswith(("–æ–≤", "–µ–≤", "—ñ–π", "–∏—á", "–∏–º", "—Ä–æ", "–∫–æ", "–∞–Ω", "—ñ–Ω")):
                return len(token) >= 4

        return False

    def _create_person_signals(
        self, persons_core: List[List[str]]
    ) -> List[PersonSignal]:
        """–°–æ–∑–¥–∞–µ—Ç PersonSignal –æ–±—ä–µ–∫—Ç—ã –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤."""
        persons = []
        for person_tokens in persons_core:
            person = PersonSignal(
                core=person_tokens,
                full_name=" ".join(person_tokens),
                confidence=ConfidenceScoring.BASE_CONFIDENCE,
                evidence=["name_pattern"],
            )
            persons.append(person)
        return persons

    def _create_organization_signals(
        self, text: str, organizations_core: List[str], persons_core: List[List[str]]
    ) -> List[OrganizationSignal]:
        """–°–æ–∑–¥–∞–µ—Ç OrganizationSignal –æ–±—ä–µ–∫—Ç—ã –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º."""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –∏ —Å–æ–∑–¥–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        legal_forms_found = self._extract_legal_forms(text, organizations_core, persons_core)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ —Ñ–æ—Ä–º–∞–º–∏
        org_dict = {}

        # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        for org_core in organizations_core:
            org_dict[org_core.upper()] = OrganizationSignal(
                core=org_core.upper(),
                confidence=ConfidenceScoring.BASE_CONFIDENCE,
                evidence=["org_core"],
            )

        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º–∞—Ö
        for legal_org in legal_forms_found:
            core_key = legal_org["core"]
            if core_key in org_dict:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
                org_dict[core_key].legal_form = legal_org["legal_form"]
                org_dict[core_key].full = legal_org["full"]
                org_dict[core_key].evidence.extend(legal_org["evidence"])
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
                org_dict[core_key] = OrganizationSignal(
                    core=core_key,
                    legal_form=legal_org["legal_form"],
                    full=legal_org["full"],
                    confidence=ConfidenceScoring.BASE_CONFIDENCE,
                    evidence=legal_org["evidence"],
                )

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è precision
        self._enrich_with_context_signals(text, list(org_dict.values()))

        return list(org_dict.values())

    def _enrich_with_context_signals(self, text: str, organizations: List[OrganizationSignal]):
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è precision.

        –ò—â–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –¥–µ–ª–æ–≤—É—é/–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å:
        - –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã: –±–∞–Ω–∫, –∫—Ä–µ–¥–∏—Ç, —Å—á—ë—Ç, –ø–ª–∞—Ç–µ–∂
        - –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ: –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ, –∫–æ–º–ø–∞–Ω–∏—è, —Ñ–∏—Ä–º–∞
        - –ê–¥—Ä–µ—Å–Ω—ã–µ: —É–ª–∏—Ü–∞, –ø—Ä–æ—Å–ø–µ–∫—Ç, –≥–æ—Ä–æ–¥, –æ—Ñ–∏—Å
        - –î–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å: —É—Å–ª—É–≥–∏, —Ç–æ–≤–∞—Ä—ã, –¥–æ–≥–æ–≤–æ—Ä, –ø–æ—Å—Ç–∞–≤–∫–∞
        """
        if not organizations or not text:
            return

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ-–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ) —Å —É—á—ë—Ç–æ–º —Å–ª–æ–≤–æ—Ñ–æ—Ä–º
        context_patterns = {
            "financial": r"\b(?:–±–∞–Ω–∫[–∞-—è]*|bank|–∫—Ä–µ–¥–∏—Ç[–∞-—è]*|credit|—Å—á–µ—Ç[–∞-—è]*|account|–ø–ª–∞—Ç–µ–∂[–∞-—è]*|payment|–ø–µ—Ä–µ–≤–æ–¥[–∞-—è]*|transfer)\b",
            "business": r"\b(?:–ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏[–∞-—è]*|enterprise|–∫–æ–º–ø–∞–Ω–∏[–∞-—è]*|company|—Ñ–∏—Ä–º[–∞-—è]*|firm|–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏[–∞-—è]*|organization|–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏[–∞-—è]*|corporation)\b",
            "address": r"\b(?:—É–ª–∏—Ü[–∞-—è]*|—É–ª\.|–ø—Ä–æ—Å–ø–µ–∫—Ç[–∞-—è]*|–ø—Ä–æ—Å–ø\.|–≥–æ—Ä–æ–¥[–∞-—è]*|–≥\.|–æ—Ñ–∏—Å[–∞-—è]*|office|–∞–¥—Ä–µ—Å[–∞-—è]*|address)\b",
            "activity": r"\b(?:—É—Å–ª—É–≥[–∞-—è]*|services|—Ç–æ–≤–∞—Ä[–∞-—è]*|goods|–¥–æ–≥–æ–≤–æ—Ä[–∞-—è]*|contract|–ø–æ—Å—Ç–∞–≤–∫[–∞-—è]*|supply|—Ä–µ–∞–ª–∏–∑–∞—Ü–∏[–∞-—è]*|implementation)\b"
        }

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        compiled_patterns = {}
        for category, pattern in context_patterns.items():
            try:
                compiled_patterns[category] = re.compile(pattern, re.IGNORECASE)
            except re.error:
                continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—É—é –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
        for org in organizations:
            context_matches = []

            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã –≤ —Ç–µ–∫—Å—Ç–µ
            for category, pattern in compiled_patterns.items():
                matches = pattern.findall(text)
                if matches:
                    context_matches.extend([(category, match) for match in matches])

            # –î–æ–±–∞–≤–ª—è–µ–º evidence –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
            if context_matches:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                found_categories = set(match[0] for match in context_matches)

                for category in found_categories:
                    if category == "financial":
                        org.evidence.append("financial_context")
                    elif category == "business":
                        org.evidence.append("business_context")
                    elif category == "address":
                        org.evidence.append("address_context")
                    elif category == "activity":
                        org.evidence.append("activity_context")

                self.logger.debug(
                    f"Organization '{org.core}' enhanced with context signals: {found_categories}"
                )

    def _enrich_with_identifiers(
        self,
        text: str,
        persons: List[PersonSignal],
        organizations: List[OrganizationSignal],
        normalization_result: Optional[Dict[str, Any]] = None,
    ):
        """–û–±–æ–≥–∞—â–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞–º–∏."""
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏–∑ trace –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
        trace_ids = self._extract_ids_from_normalization_trace(normalization_result, text)

        # 2. –î–æ–ø–æ–ª–Ω—è–µ–º regex-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞ (fallback –∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ)
        org_ids = self.identifier_extractor.extract_organization_ids(text)
        person_ids = self.identifier_extractor.extract_person_ids(text)

        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º ID –∏–∑ trace —Å regex-–∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏
        all_org_ids = trace_ids.get('organization_ids', []) + org_ids
        all_person_ids = trace_ids.get('person_ids', []) + person_ids

        # 4. –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é
        unique_org_ids = self._deduplicate_ids(all_org_ids)
        unique_person_ids = self._deduplicate_ids(all_person_ids)

        self.logger.debug(f"[CHECK] ID ENRICHMENT: Found {len(unique_person_ids)} person IDs, {len(unique_org_ids)} org IDs")
        if unique_person_ids:
            self.logger.debug(f"[CHECK] PERSON IDS: {[(p.get('type'), p.get('value'), p.get('source')) for p in unique_person_ids[:3]]}")

        # 5. –û–±–æ–≥–∞—â–∞–µ–º –ø–µ—Ä—Å–æ–Ω –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ ID
        self._enrich_organizations_with_ids(organizations, unique_org_ids)
        self._enrich_persons_with_ids(persons, unique_person_ids)

        # 6. FAST PATH: –ü—Ä–æ–≤–µ—Ä—è–µ–º INN cache –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Å–∞–Ω–∫—Ü–∏–π (–ø–æ—Å–ª–µ –æ–±–æ–≥–∞—â–µ–Ω–∏—è ID)
        self._check_sanctioned_inn_cache(unique_person_ids, unique_org_ids, persons, organizations)

    def _enrich_with_birthdates(self, text: str, persons: List[PersonSignal]):
        """–û–±–æ–≥–∞—â–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω—ã –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏ —Ä–æ–∂–¥–µ–Ω–∏—è."""
        birthdates = self.birthdate_extractor.extract(text)
        self._enrich_persons_with_birthdates(persons, birthdates, text)

    def _build_result(
        self, persons: List[PersonSignal], organizations: List[OrganizationSignal]
    ) -> Dict[str, Any]:
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç."""
        # Calculate overall confidence as average of entity confidences
        all_confidences = []
        if persons:
            all_confidences.extend([p.confidence for p in persons])
        if organizations:
            all_confidences.extend([o.confidence for o in organizations])

        overall_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0.0

        # Check for sanctioned IDs (FAST PATH markers)
        sanctioned_id_found = False
        sanctioned_count = 0

        for person in persons:
            for id_info in person.ids:
                if id_info.get('sanctioned', False):
                    sanctioned_id_found = True
                    sanctioned_count += 1

        for org in organizations:
            for id_info in org.ids:
                if id_info.get('sanctioned', False):
                    sanctioned_id_found = True
                    sanctioned_count += 1

        result = {
            "persons": [self._person_to_dict(p) for p in persons],
            "organizations": [self._org_to_dict(o) for o in organizations],
            "extras": {"dates": [], "amounts": []},
            "confidence": overall_confidence,
        }

        # Add fast path sanctions info
        if sanctioned_id_found:
            result["fast_path_sanctions"] = {
                "sanctioned_ids_found": sanctioned_count,
                "cache_hit": True
            }
            self.logger.warning(f"üö® FAST PATH: {sanctioned_count} sanctioned IDs found via cache")

        return result

    def _extract_legal_forms(
        self, text: str, organizations_core: List[str], persons_core: List[List[str]]
    ) -> List[Dict]:
        """
        –î–µ—Ç–µ–∫—Ç–æ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º –∏ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            organizations_core: –Ø–¥—Ä–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å legal_form –∏ full
        """
        from ...data.patterns.legal_forms import (
            LEGAL_FORM_REGEX,
            ORGANIZATION_NAME_REGEX,
            QUOTED_CORE_REGEX,
            normalize_legal_form,
        )

        organizations = []
        # Prepare flattened set of person tokens to avoid mixing persons into organization names
        person_tokens_upper = set()
        try:
            for person in persons_core or []:
                for tok in person or []:
                    if isinstance(tok, str) and tok.strip():
                        person_tokens_upper.add(tok.strip().upper())
        except Exception:
            person_tokens_upper = set()

        # –ò—â–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –≤ —Ç–µ–∫—Å—Ç–µ
        for legal_match in LEGAL_FORM_REGEX.finditer(text):
            legal_form_raw = legal_match.group(0)
            legal_form_normalized = normalize_legal_form(legal_form_raw)

            # –ü–æ–∑–∏—Ü–∏—è —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã
            form_start = legal_match.start()
            form_end = legal_match.end()

            # –ò—â–µ–º quoted core —Ä—è–¥–æ–º —Å —Ñ–æ—Ä–º–æ–π (¬±100 —Å–∏–º–≤–æ–ª–æ–≤)
            search_start = max(0, form_start - 100)
            search_end = min(len(text), form_end + 100)
            search_region = text[search_start:search_end]

            core = None
            full = None
            evidence = ["legal_form_hit"]

            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ quoted –Ω–∞–∑–≤–∞–Ω–∏–µ
            quoted_matches = list(QUOTED_CORE_REGEX.finditer(search_region))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º quoted matches –ø–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—é –¥–æ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã
            def distance_to_legal_form(quoted_match):
                # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç quoted match –¥–æ legal form –≤ search_region
                quoted_start_in_search = quoted_match.start()
                quoted_end_in_search = quoted_match.end()

                # –ü–æ–∑–∏—Ü–∏—è legal form –≤ search_region
                legal_start_in_search = form_start - search_start
                legal_end_in_search = form_end - search_start

                # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏
                quoted_center = (quoted_start_in_search + quoted_end_in_search) / 2
                legal_center = (legal_start_in_search + legal_end_in_search) / 2
                return abs(quoted_center - legal_center)

            quoted_matches.sort(key=distance_to_legal_form)

            for quoted_match in quoted_matches:
                quoted_core = quoted_match.group(1).strip()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —ç—Ç–æ —è–¥—Ä–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
                if quoted_core.upper() in [org.upper() for org in organizations_core]:
                    core = quoted_core.upper()
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                    normalized_core = next(
                        org
                        for org in organizations_core
                        if org.upper() == quoted_core.upper()
                    )
                    full = f'{legal_form_normalized} "{normalized_core}"'
                    evidence.append("quoted_core")
                    evidence.append("norm_match")
                    break
                elif len(quoted_core) >= 3:  # –ú–∏–Ω–∏–º—É–º 3 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
                    core = quoted_core.upper()
                    full = (
                        f'{legal_form_normalized} "{core}"'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º uppercase core
                    )
                    evidence.append("quoted_core")
                    break

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –∫–∞–≤—ã—á–∫–∞—Ö, –∏—â–µ–º —Ä—è–¥–æ–º —Å —Ñ–æ—Ä–º–æ–π
            if not core:
                # 1) –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –°–ü–†–ê–í–ê –æ—Ç —é—Ä. —Ñ–æ—Ä–º—ã (–ø–∞—Ç—Ç–µ—Ä–Ω: `–¢–û–í <–Ω–∞–∑–≤–∞–Ω–∏–µ>`)
                right_region = text[form_end : min(len(text), form_end + 50)].strip()
                if right_region:
                    words = right_region.split()
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏/—Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ
                    skip_heads = {"–≤—ñ–¥", "–æ—Ç", "–¥–ª—è", "–∑–∞", "–Ω–∞", "of", "for", "by", "to"}
                    while words and words[0].lower() in skip_heads:
                        words.pop(0)
                    if words:
                        for i in range(min(5, len(words)), 0, -1):
                            candidate = " ".join(words[:i]).strip()
                            if (
                                len(candidate) >= 3
                                and any(c.isalpha() for c in candidate)
                                and len(candidate) <= 50
                            ):
                                # Skip candidate if it heavily overlaps with person tokens (>=2 tokens)
                                cand_words = [w for w in candidate.replace('"', ' ').split() if w]
                                overlap = sum(1 for w in cand_words if w.upper() in person_tokens_upper)
                                if overlap >= 2:
                                    continue
                                if candidate.upper() in [org.upper() for org in organizations_core]:
                                    core = candidate.upper()
                                    normalized_core = next(
                                        org for org in organizations_core if org.upper() == candidate.upper()
                                    )
                                    full = f"{legal_form_normalized} {normalized_core}"
                                    evidence.extend(["adjacent_name", "norm_match"])
                                    break
                                else:
                                    core = candidate.upper()
                                    full = f"{legal_form_normalized} {candidate}"
                                    evidence.append("adjacent_name")
                                    break

                # 2) –ï—Å–ª–∏ —Å–ø—Ä–∞–≤–∞ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø—Ä–æ–±—É–µ–º —Å–ª–µ–≤–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 1‚Äì3 —Å–ª–æ–≤–∞)
                if not core:
                    left_region = text[max(0, form_start - 50) : form_start].strip()
                    if left_region:
                        words = left_region.split()
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏/—Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Ü–µ –ª–µ–≤–æ–π –æ–±–ª–∞—Å—Ç–∏
                        skip_tails = {"–≤—ñ–¥", "–æ—Ç", "–¥–ª—è", "–∑–∞", "–Ω–∞", "of", "for", "by", "to"}
                        while words and words[-1].lower() in skip_tails:
                            words.pop()
                        if words:
                            for i in range(min(3, len(words)), 0, -1):
                                candidate = " ".join(words[-i:]).strip()
                                if (
                                    len(candidate) >= 3
                                    and any(c.isalpha() for c in candidate)
                                    and len(candidate) <= 50
                                ):
                                    # Skip candidate if it heavily overlaps with person tokens (>=2 tokens)
                                    cand_words = [w for w in candidate.replace('"', ' ').split() if w]
                                    overlap = sum(1 for w in cand_words if w.upper() in person_tokens_upper)
                                    if overlap >= 2:
                                        continue
                                    if candidate.upper() in [org.upper() for org in organizations_core]:
                                        core = candidate.upper()
                                        normalized_core = next(
                                            org for org in organizations_core if org.upper() == candidate.upper()
                                        )
                                        full = f"{legal_form_normalized} {normalized_core}"
                                        evidence.extend(["adjacent_name", "norm_match"])
                                        break
                                    else:
                                        core = candidate.upper()
                                        full = f"{legal_form_normalized} {candidate}"
                                        evidence.append("adjacent_name")
                                        break

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ core, –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
            if core:
                org_info = {
                    "core": core,
                    "legal_form": legal_form_normalized,
                    "full": full,
                    "evidence": evidence,
                    "legal_form_raw": legal_form_raw,
                }
                organizations.append(org_info)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ core
        seen_cores = set()
        unique_orgs = []
        for org in organizations:
            if org["core"] not in seen_cores:
                seen_cores.add(org["core"])
                unique_orgs.append(org)

        return unique_orgs

    def _extract_org_ids(self, text: str) -> List[Dict]:
        """–î–µ—Ç–µ–∫—Ç–æ—Ä –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö ID"""
        from ...data.patterns.identifiers import (
            get_compiled_patterns_cached,
            get_validation_function,
            normalize_identifier,
        )

        org_id_types = {
            "edrpou",
            "inn_ru",
            "ogrn",
            "ogrnip",
            "kpp",
            "vat_eu",
            "lei",
            "ein",
        }

        found_ids = []

        for pattern, compiled_regex in get_compiled_patterns_cached():
            if pattern.type not in org_id_types:
                continue

            for match in compiled_regex.finditer(text):
                raw_value = match.group(1)
                normalized_value = normalize_identifier(raw_value, pattern.type)

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
                validator = get_validation_function(pattern.type)
                is_valid = True
                if validator:
                    is_valid = validator(normalized_value)

                confidence = 0.9 if is_valid else 0.6

                id_info = {
                    "type": pattern.type,
                    "value": normalized_value,
                    "raw": match.group(0),  # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    "name": pattern.name,
                    "confidence": confidence,
                    "position": match.span(),
                    "valid": is_valid,
                }
                found_ids.append(id_info)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ value
        seen_values = set()
        unique_ids = []
        for id_info in found_ids:
            if id_info["value"] not in seen_values:
                seen_values.add(id_info["value"])
                unique_ids.append(id_info)

        return unique_ids

    def _extract_person_ids(self, text: str) -> List[Dict]:
        """–î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—á–Ω—ã—Ö ID"""
        from ...data.patterns.identifiers import (
            get_compiled_patterns_cached,
            get_validation_function,
            normalize_identifier,
        )

        person_id_types = {"inn_ua", "inn_ru", "snils", "ssn", "passport_ua"}

        found_ids = []

        for pattern, compiled_regex in get_compiled_patterns_cached():
            if pattern.type not in person_id_types:
                continue

            for match in compiled_regex.finditer(text):
                raw_value = match.group(1)
                normalized_value = normalize_identifier(raw_value, pattern.type)

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
                validator = get_validation_function(pattern.type)
                is_valid = True
                if validator:
                    is_valid = validator(normalized_value)

                confidence = 0.9 if is_valid else 0.6

                id_info = {
                    "type": pattern.type,
                    "value": normalized_value,
                    "raw": match.group(0),  # –ü–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    "name": pattern.name,
                    "confidence": confidence,
                    "position": match.span(),
                    "valid": is_valid,
                }
                found_ids.append(id_info)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ value
        seen_values = set()
        unique_ids = []
        for id_info in found_ids:
            if id_info["value"] not in seen_values:
                seen_values.add(id_info["value"])
                unique_ids.append(id_info)

        return unique_ids

    def _extract_birthdates(self, text: str) -> List[Dict]:
        """–î–µ—Ç–µ–∫—Ç–æ—Ä –¥–∞—Ç —Ä–æ–∂–¥–µ–Ω–∏—è"""
        from ...data.patterns.dates import extract_birthdates_from_text

        return extract_birthdates_from_text(text)

    def _enrich_organizations_with_ids(
        self, organizations: List[OrganizationSignal], org_ids: List[Dict]
    ):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ ID"""
        if not org_ids:
            return

        for org in organizations:
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ ID –¥–ª—è —ç—Ç–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            for id_info in org_ids:
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ ID
                # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—É—é –ª–æ–≥–∏–∫—É —Å–≤—è–∑—ã–≤–∞–Ω–∏—è
                org.ids.append(
                    {
                        "type": id_info["type"],
                        "value": id_info["value"],
                        "raw": id_info["raw"],
                        "confidence": id_info["confidence"],
                        "valid": id_info["valid"],
                    }
                )

                # –î–æ–±–∞–≤–ª—è–µ–º evidence (—Å–∫–æ—Ä–∏–Ω–≥ –±—É–¥–µ—Ç –≤ _score_entities)
                if id_info["valid"]:
                    org.evidence.append(f"valid_{id_info['type']}")
                else:
                    org.evidence.append(f"invalid_{id_info['type']}")

    def _enrich_persons_with_ids(
        self, persons: List[PersonSignal], person_ids: List[Dict]
    ):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ ID —Å proximity matching"""
        if not person_ids or not persons:
            return

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º proximity matching –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ª–æ–≥–∏–∫–µ –¥–ª—è –¥–∞—Ç —Ä–æ–∂–¥–µ–Ω–∏—è
        self._link_ids_to_persons_by_proximity(persons, person_ids, self._current_text)

    def _link_ids_to_persons_by_proximity(
        self, persons: List[PersonSignal], person_ids: List[Dict], text: str
    ):
        """–°–≤—è–∑—ã–≤–∞–Ω–∏–µ ID —Å –ø–µ—Ä—Å–æ–Ω–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–ª–∏–∑–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not person_ids or not persons or not text:
            return

        # Check if text is mixed language for more lenient proximity matching
        is_mixed_language = self._is_mixed_language_text(text)

        # –ù–∞–π–¥–µ–º –ø–æ–∑–∏—Ü–∏–∏ –≤—Å–µ—Ö –ø–µ—Ä—Å–æ–Ω –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è proximity matching
        person_positions = []
        for person in persons:
            # –°–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ–µ –∏–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            full_name = " ".join(person.core)

            # –ò—â–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∏–º–µ–Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            import re

            name_pattern = re.escape(full_name)
            matches = list(re.finditer(name_pattern, text, re.IGNORECASE))

            if matches:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                start_pos = matches[0].start()
                end_pos = matches[0].end()
                person_positions.append(
                    {
                        "person": person,
                        "start": start_pos,
                        "end": end_pos,
                        "center": (start_pos + end_pos) // 2,
                    }
                )

        # –ê—Å—Å–æ—Ü–∏–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π ID —Å –±–ª–∏–∂–∞–π—à–µ–π –ø–µ—Ä—Å–æ–Ω–æ–π
        used_ids = set()

        for id_info in person_ids:
            if "position" not in id_info:
                # –ï—Å–ª–∏ –ø–æ–∑–∏—Ü–∏—è ID –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ –≤—Å–µ–º –ø–µ—Ä—Å–æ–Ω–∞–º
                # (fallback –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                self._assign_id_to_all_persons(persons, id_info)
                continue

            id_start = id_info["position"][0]
            id_end = id_info["position"][1]
            id_center = (id_start + id_end) // 2

            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à—É—é –ø–µ—Ä—Å–æ–Ω—É
            closest_person = None
            min_distance = float("inf")

            for person_pos in person_positions:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ ID –Ω–µ –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
                id_key = f"{id_info['type']}_{id_info['value']}"
                if id_key in used_ids:
                    continue

                person_center = person_pos["center"]

                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏
                distance = abs(id_center - person_center)

                # –î–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                max_distance = 500 if is_mixed_language else 300
                
                # –û–≥—Ä–∞–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º
                # (–±–æ–ª—å—à–µ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞, —Ç.–∫. ID –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –¥—Ä—É–≥–æ–π —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞)
                if distance < max_distance and distance < min_distance:
                    min_distance = distance
                    closest_person = person_pos["person"]

                    self.logger.debug(
                        f"ID {id_info['type']}:{id_info['value']} distance {distance} to person "
                        f"{''.join(person_pos['person'].core)} (new closest)"
                    )

            # –ù–∞–∑–Ω–∞—á–∞–µ–º ID –±–ª–∏–∂–∞–π—à–µ–π –ø–µ—Ä—Å–æ–Ω–µ
            if closest_person:
                id_key = f"{id_info['type']}_{id_info['value']}"
                if id_key not in used_ids:
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –ø–µ—Ä—Å–æ–Ω–∞ —É–∂–µ –∏–º–µ–µ—Ç ID —Ç–æ–≥–æ –∂–µ —Ç–∏–ø–∞,
                    # —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–º –æ—à–∏–±–∫–∏ –∏–ª–∏ –Ω–∞–ª–∏—á–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
                    existing_id_types = {existing_id["type"] for existing_id in closest_person.ids}

                    if id_info["type"] in existing_id_types:
                        self.logger.debug(
                            f"Person {''.join(closest_person.core)} already has ID of type "
                            f"{id_info['type']}, possible multiple entities"
                        )
                        # –ù–µ –Ω–∞–∑–Ω–∞—á–∞–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–π —Ç–∏–ø ID —Ç–æ–π –∂–µ –ø–µ—Ä—Å–æ–Ω–µ
                        # –≠—Ç–æ ID –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –¥–ª—è fallback –ª–æ–≥–∏–∫–∏
                    else:
                        self._assign_id_to_person(closest_person, id_info)
                        used_ids.add(id_key)

                        self.logger.debug(
                            f"Linked ID {id_info['type']}:{id_info['value']} to person "
                            f"{''.join(closest_person.core)} (distance: {min_distance})"
                        )

        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å –ø–µ—Ä—Å–æ–Ω—ã –±–µ–∑ ID, –Ω–æ –µ—Å—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ ID,
        # –ø—Ä–∏–º–µ–Ω—è–µ–º fallback –ª–æ–≥–∏–∫—É –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è ID
        remaining_ids = [
            id_info for id_info in person_ids
            if f"{id_info['type']}_{id_info['value']}" not in used_ids
        ]

        if remaining_ids:
            self.logger.debug(f"Applying fallback logic for {len(remaining_ids)} unlinked IDs")

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—É—é fallback –ª–æ–≥–∏–∫—É –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö:
            # 1. –ï—Å–ª–∏ –ø–µ—Ä—Å–æ–Ω –±–µ–∑ ID –±–æ–ª—å—à–µ —á–µ–º ID (–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ) –ò
            #    –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ID –Ω–µ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–µ—Ä—Å–æ–Ω
            # 2. –ò–ª–∏ –µ—Å–ª–∏ —É –Ω–∞—Å –ø—Ä–æ—Å—Ç–∞—è —Å–∏—Ç—É–∞—Ü–∏—è: 1 –ø–µ—Ä—Å–æ–Ω–∞, 1 ID, –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –Ω–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ
            persons_without_ids = [p for p in persons if not p.ids]

            # –î–ª—è —Å–ª—É—á–∞—è 1 –ø–µ—Ä—Å–æ–Ω–∞ + 1 ID –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            can_apply_single_fallback = True
            if len(persons_without_ids) == 1 and len(remaining_ids) == 1:
                id_info = remaining_ids[0]
                if "position" in id_info and person_positions:
                    person_pos = person_positions[0]  # –ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞
                    id_center = (id_info["position"][0] + id_info["position"][1]) // 2
                    person_center = person_pos["center"]
                    distance = abs(id_center - person_center)

                    # –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–æ–µ (>500), –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ–º fallback
                    if distance > 500:
                        can_apply_single_fallback = False
                        self.logger.debug(
                            f"Skipping single fallback: distance {distance} > 500 chars"
                        )

            # –£—Å–ª–æ–≤–∏–µ 1: –ü–µ—Ä—Å–æ–Ω —Å—Ç—Ä–æ–≥–æ –±–æ–ª—å—à–µ —á–µ–º ID (–º–æ–∂–Ω–æ –Ω–∞–∑–Ω–∞—á–∏—Ç—å –≤—Å–µ ID –ø–µ—Ä—Å–æ–Ω–∞–º)
            # –£—Å–ª–æ–≤–∏–µ 2: –ü—Ä–æ—Å—Ç–æ–π —Å–ª—É—á–∞–π 1:1 —Å –ø–æ–¥—Ö–æ–¥—è—â–∏–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º
            if ((len(persons_without_ids) > len(remaining_ids) and len(remaining_ids) > 0) or
                (len(persons_without_ids) == 1 and len(remaining_ids) == 1 and can_apply_single_fallback)):

                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è ID –∫ –ø–µ—Ä—Å–æ–Ω–∞–º –±–µ–∑ ID (–ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –ø–µ—Ä—Å–æ–Ω—É)
                for i, id_info in enumerate(remaining_ids):
                    if i < len(persons_without_ids):
                        self._assign_id_to_person(persons_without_ids[i], id_info)
                        self.logger.debug(f"Fallback: assigned ID {id_info['type']}:{id_info['value']} to person")
            else:
                # –°–ª–∏—à–∫–æ–º —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–æ –Ω–∞–∑–Ω–∞—á–∞—Ç—å ID - –≤–æ–∑–º–æ–∂–Ω–æ, –æ–Ω–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –Ω–µ —Å–≤—è–∑–∞–Ω—ã
                # –ù–∞–ø—Ä–∏–º–µ—Ä: 1 –ø–µ—Ä—Å–æ–Ω–∞ –∏ 2+ ID - –Ω–µ—è—Å–Ω–æ, –∫–∞–∫–∏–µ ID –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –ø–µ—Ä—Å–æ–Ω–µ
                self.logger.debug(
                    f"Skipping fallback assignment: {len(remaining_ids)} remaining IDs, "
                    f"{len(persons_without_ids)} persons without IDs"
                )

    def _assign_id_to_person(self, person: PersonSignal, id_info: Dict):
        """–ü—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç ID –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–µ—Ä—Å–æ–Ω–µ"""
        person.ids.append(
            {
                "type": id_info["type"],
                "value": id_info["value"],
                "raw": id_info["raw"],
                "confidence": id_info["confidence"],
                "valid": id_info["valid"],
            }
        )

        # –î–æ–±–∞–≤–ª—è–µ–º evidence (—Å–∫–æ—Ä–∏–Ω–≥ –±—É–¥–µ—Ç –≤ _score_entities)
        if id_info["valid"]:
            person.evidence.append(f"valid_{id_info['type']}")
        else:
            person.evidence.append(f"invalid_{id_info['type']}")

    def _assign_id_to_all_persons(self, persons: List[PersonSignal], id_info: Dict):
        """Fallback: –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ—Ç ID –≤—Å–µ–º –ø–µ—Ä—Å–æ–Ω–∞–º (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)"""
        for person in persons:
            self._assign_id_to_person(person, id_info)

    def _enrich_persons_with_birthdates(
        self, persons: List[PersonSignal], birthdates: List[Dict], text: str
    ):
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏ —Ä–æ–∂–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–ª–∏–∑–æ—Å—Ç–∏ –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not birthdates or not persons:
            return

        # –ù–∞–π–¥–µ–º –ø–æ–∑–∏—Ü–∏–∏ –≤—Å–µ—Ö –ø–µ—Ä—Å–æ–Ω –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è proximity matching
        person_positions = []
        for person in persons:
            # –°–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª–Ω–æ–µ –∏–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            full_name = " ".join(person.core)

            # –ò—â–µ–º –≤—Å–µ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∏–º–µ–Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            import re

            name_pattern = re.escape(full_name)
            matches = list(re.finditer(name_pattern, text, re.IGNORECASE))

            if matches:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                start_pos = matches[0].start()
                end_pos = matches[0].end()
                person_positions.append(
                    {
                        "person": person,
                        "start": start_pos,
                        "end": end_pos,
                        "center": (start_pos + end_pos) // 2,
                    }
                )

        # –ê—Å—Å–æ—Ü–∏–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –¥–∞—Ç—É —Ä–æ–∂–¥–µ–Ω–∏—è —Å –±–ª–∏–∂–∞–π—à–µ–π –ø–µ—Ä—Å–æ–Ω–æ–π
        used_birthdates = set()

        for date_info in birthdates:
            date_start = date_info["position"][0]
            date_end = date_info["position"][1]
            date_center = (date_start + date_end) // 2

            # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à—É—é –ø–µ—Ä—Å–æ–Ω—É
            closest_person = None
            min_distance = float("inf")

            for person_pos in person_positions:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞—Ç–∞ –Ω–µ –±—ã–ª–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞
                if date_info["raw"] in used_birthdates:
                    continue

                person_center = person_pos["center"]

                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏
                distance = abs(date_center - person_center)

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É—á–∏—Ç—ã–≤–∞–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ –¥–∞—Ç–∞ –≤ "—Ä–∞–∑—É–º–Ω–æ–º" –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç –ø–µ—Ä—Å–æ–Ω—ã
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –≤ 200 —Å–∏–º–≤–æ–ª–æ–≤
                if distance < 200 and distance < min_distance:
                    min_distance = distance
                    closest_person = person_pos["person"]

            # –ù–∞–∑–Ω–∞—á–∞–µ–º –¥–∞—Ç—É –±–ª–∏–∂–∞–π—à–µ–π –ø–µ—Ä—Å–æ–Ω–µ
            if closest_person and date_info["raw"] not in used_birthdates:
                closest_person.dob = date_info["iso_format"]  # ISO —Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD
                closest_person.dob_raw = date_info["raw"]  # –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
                closest_person.evidence.append("birthdate_found")
                used_birthdates.add(date_info["raw"])

        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å –ø–µ—Ä—Å–æ–Ω—ã –±–µ–∑ –¥–∞—Ç, –Ω–æ –µ—Å—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞—Ç—ã,
        # –Ω–∞–∑–Ω–∞—á–∞–µ–º –∏—Ö –≤ –ø–æ—Ä—è–¥–∫–µ –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
        remaining_dates = [d for d in birthdates if d["raw"] not in used_birthdates]
        persons_without_dates = [p for p in persons if not p.dob]

        for i, person in enumerate(persons_without_dates):
            if i < len(remaining_dates):
                date_info = remaining_dates[i]
                person.dob = date_info["iso_format"]
                person.dob_raw = date_info["raw"]
                person.evidence.append("birthdate_found")

    def _score_entities(
        self, persons: List[PersonSignal], organizations: List[OrganizationSignal]
    ):
        """
        –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥ —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö evidence.

        –ê–ª–≥–æ—Ä–∏—Ç–º —Å–∫–æ—Ä–∏–Ω–≥–∞:
        - –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: 0.5
        - –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞ —Å –≤–∞–ª–∏–¥–Ω—ã–º core: +0.3
        - –í–∞–ª–∏–¥–Ω—ã–π ID: +0.2
        - –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π ID: +0.1
        - –î–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è: +0.15
        - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ evidence: –±–æ–Ω—É—Å +0.05 –∑–∞ –∫–∞–∂–¥—ã–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π
        """

        # –°–∫–æ—Ä–∏–Ω–≥ –ø–µ—Ä—Å–æ–Ω
        for person in persons:
            base_confidence = ConfidenceScoring.BASE_CONFIDENCE
            bonus = 0.0
            evidence_count = len(person.evidence)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º evidence –¥–ª—è –±–æ–Ω—É—Å–æ–≤
            for ev in person.evidence:
                if ev == "birthdate_found":
                    bonus += ConfidenceScoring.BIRTHDATE_BONUS
                elif ev.startswith("valid_"):
                    bonus += ConfidenceScoring.VALID_ID_BONUS
                elif ev.startswith("invalid_"):
                    bonus += ConfidenceScoring.INVALID_ID_BONUS
                elif ev == "name_pattern":
                    bonus += ConfidenceScoring.NAME_PATTERN_BONUS

            # –ë–æ–Ω—É—Å –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ evidence
            if evidence_count > 1:
                multi_bonus = min(
                    ConfidenceScoring.PERSON_MAX_MULTI_BONUS,
                    (evidence_count - 1) * ConfidenceScoring.MULTI_EVIDENCE_INCREMENT,
                )
                bonus += multi_bonus

            # –û–±–Ω–æ–≤–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º—É–º–∞
            person.confidence = min(
                ConfidenceScoring.MAX_CONFIDENCE, base_confidence + bonus
            )

        # –°–∫–æ—Ä–∏–Ω–≥ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        for org in organizations:
            base_confidence = ConfidenceScoring.BASE_CONFIDENCE
            bonus = 0.0
            evidence_count = len(org.evidence)

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º evidence –¥–ª—è –±–æ–Ω—É—Å–æ–≤
            for ev in org.evidence:
                if ev == "legal_form_hit":
                    bonus += ConfidenceScoring.LEGAL_FORM_BONUS
                elif ev == "quoted_core":
                    bonus += ConfidenceScoring.QUOTED_CORE_BONUS
                elif ev == "norm_match":
                    bonus += ConfidenceScoring.NORM_MATCH_BONUS
                elif ev == "adjacent_name":
                    bonus += ConfidenceScoring.ADJACENT_NAME_BONUS
                elif ev.startswith("valid_"):
                    bonus += ConfidenceScoring.VALID_ID_BONUS
                elif ev.startswith("invalid_"):
                    bonus += ConfidenceScoring.INVALID_ID_BONUS
                elif ev == "org_core":
                    bonus += ConfidenceScoring.ORG_CORE_BONUS
                # –ù–æ–≤—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –±–æ–Ω—É—Å—ã
                elif ev in ["financial_context", "business_context", "activity_context"]:
                    bonus += ConfidenceScoring.CONTEXT_BONUS
                elif ev == "address_context":
                    bonus += ConfidenceScoring.ADDRESS_BONUS

            # –ë–æ–Ω—É—Å –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ evidence
            if evidence_count > 1:
                multi_bonus = min(
                    ConfidenceScoring.ORG_MAX_MULTI_BONUS,
                    (evidence_count - 1) * ConfidenceScoring.MULTI_EVIDENCE_INCREMENT,
                )
                bonus += multi_bonus

            # –û–±–Ω–æ–≤–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –º–∞–∫—Å–∏–º—É–º–∞
            org.confidence = min(
                ConfidenceScoring.MAX_CONFIDENCE, base_confidence + bonus
            )

    def _person_to_dict(self, person: PersonSignal) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PersonSignal –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            "core": person.core,
            "full_name": person.full_name,
            "dob": person.dob,
            "dob_raw": person.dob_raw,
            "ids": person.ids,
            "confidence": person.confidence,
            "evidence": person.evidence,
        }

    def _org_to_dict(self, org: OrganizationSignal) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è OrganizationSignal –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            "core": org.core,
            "legal_form": org.legal_form,
            "full": org.full,
            "ids": org.ids,
            "confidence": org.confidence,
            "evidence": org.evidence,
        }

    def _empty_result(self) -> Dict[str, Any]:
        """–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ –∏–ª–∏ –ø—É—Å—Ç–æ–º –≤–≤–æ–¥–µ"""
        return {
            "persons": [],
            "organizations": [],
            "extras": {"dates": [], "amounts": []},
            "confidence": 0.0,
        }

    # Backward compatibility methods for tests
    def _extract_persons(self, text: str, normalization_result: Optional[Dict] = None) -> List[Dict]:
        """Backward compatibility method for tests"""
        persons_core, _ = self._get_entity_cores(text, normalization_result, "auto")
        person_signals = self._create_person_signals(persons_core)
        return [
            {
                "core": signal.core,
                "full_name": signal.full_name,
                "confidence": signal.confidence,
                "evidence": signal.evidence,
                "dob": signal.dob,
                "ids": signal.ids
            }
            for signal in person_signals
        ]

    def _extract_organizations(self, text: str, normalization_result: Optional[Dict] = None) -> List[Dict]:
        """Backward compatibility method for tests"""
        persons_core, organizations_core = self._get_entity_cores(text, normalization_result, "auto")
        org_signals = self._create_organization_signals(text, organizations_core, persons_core)
        return [
            {
                "core": signal.core,
                "legal_form": signal.legal_form,
                "full": signal.full,
                "confidence": signal.confidence,
                "evidence": signal.evidence,
                "ids": signal.ids
            }
            for signal in org_signals
        ]

    def _extract_persons_from_normalization(self, normalization_result: Dict) -> List[Dict]:
        """Backward compatibility method for tests"""
        return self._extract_persons("", normalization_result)

    def _extract_organizations_from_normalization(self, normalization_result: Dict) -> List[Dict]:
        """Backward compatibility method for tests"""
        return self._extract_organizations("", normalization_result)

    def _extract_organization_ids(self, text: str) -> List[Dict]:
        """Backward compatibility method for tests"""
        return self._extract_org_ids(text)

    def _extract_extras(self, text: str) -> Dict[str, List]:
        """Backward compatibility method for tests"""
        birthdates = self._extract_birthdates(text)
        return {
            "dates": birthdates,
            "amounts": []  # Amounts not implemented yet
        }

    async def extract_signals(self, text: str, normalization_result: Optional[Dict] = None,
                            language: str = "auto"):
        """Backward compatibility method for tests - returns result with object attributes"""
        result_dict = await self.extract_async(text, normalization_result, language)

        # Create a simple object wrapper for backward compatibility
        class ResultWrapper:
            def __init__(self, result_dict):
                # Convert person dicts to simple objects with attributes
                self.persons = []
                for person_dict in result_dict.get("persons", []):
                    person_obj = type('PersonObj', (), {})()
                    for key, value in person_dict.items():
                        setattr(person_obj, key, value)
                    self.persons.append(person_obj)

                # Convert organization dicts to simple objects with attributes
                self.organizations = []
                for org_dict in result_dict.get("organizations", []):
                    org_obj = type('OrgObj', (), {})()
                    for key, value in org_dict.items():
                        # Map 'full' to 'full_name' for backward compatibility
                        if key == "full":
                            setattr(org_obj, "full_name", value)
                            setattr(org_obj, key, value)  # Keep original too
                        else:
                            setattr(org_obj, key, value)

                    # Ensure 'full' attribute always exists for backward compatibility
                    if not hasattr(org_obj, 'full'):
                        setattr(org_obj, 'full', None)
                    if not hasattr(org_obj, 'full_name'):
                        setattr(org_obj, 'full_name', None)

                    self.organizations.append(org_obj)

                # Copy other attributes directly
                for key, value in result_dict.items():
                    if key not in ["persons", "organizations"]:
                        setattr(self, key, value)

                # Add backward compatibility attributes for test expectations
                # Extract numbers (IDs) from persons and organizations
                self.numbers = {}
                all_ids = []

                # Collect IDs from persons
                for person in self.persons:
                    if hasattr(person, 'ids') and person.ids:
                        all_ids.extend(person.ids)

                # Collect IDs from organizations
                for org in self.organizations:
                    if hasattr(org, 'ids') and org.ids:
                        all_ids.extend(org.ids)

                # Organize IDs by type
                for id_item in all_ids:
                    if isinstance(id_item, dict):
                        id_type = id_item.get('type', 'unknown')
                        id_value = id_item.get('value', id_item.get('raw', ''))
                        if id_type not in self.numbers:
                            self.numbers[id_type] = []
                        self.numbers[id_type].append(id_value)

                # Extract dates from persons
                self.dates = {}
                birth_dates = []

                for person in self.persons:
                    # Check for both dob and birth_date attributes for compatibility
                    dob = None
                    if hasattr(person, 'dob') and person.dob:
                        dob = person.dob
                    elif hasattr(person, 'birth_date') and person.birth_date:
                        dob = person.birth_date

                    if dob:
                        birth_dates.append(str(dob))

                # Also check extras for dates
                extras_dates = result_dict.get("extras", {}).get("dates", [])
                birth_dates.extend(extras_dates)

                if birth_dates:
                    self.dates['birth_dates'] = birth_dates

        return ResultWrapper(result_dict)

    def _extract_person_tokens(self, text: str, language: str) -> List[List[str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –ø–µ—Ä—Å–æ–Ω –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–º–µ–Ω:
        - –°–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã, –Ω–µ —è–≤–ª—è—é—â–∏–µ—Å—è —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–º–∏ —Ñ–æ—Ä–º–∞–º–∏
        - –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 2-3 –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤
        import re

        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–º–µ–Ω (2-3 –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —É–∫—Ä–∞–∏–Ω—Å–∫–∏–π –∏ —Ä—É—Å—Å–∫–∏–π –∞–ª—Ñ–∞–≤–∏—Ç—ã (—Ç–æ–ª—å–∫–æ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –≤ –Ω–∞—á–∞–ª–µ)
        name_pattern = r"\b[–ê-–Ø–Å–á–Ü–Ñ“ê][–∞-—è—ë—ó—ñ—î“ë]+(?:\s+[–ê-–Ø–Å–á–Ü–Ñ“ê][–∞-—è—ë—ó—ñ—î“ë]+){1,2}\b"

        # –¢–∞–∫–∂–µ –∏—â–µ–º –ª–∞—Ç–∏–Ω—Å–∫–∏–µ –∏–º–µ–Ω–∞
        latin_name_pattern = r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+){1,2}\b"

        found_names = []

        # –£–∫—Ä–∞–∏–Ω—Å–∫–∏–µ/—Ä—É—Å—Å–∫–∏–µ –∏–º–µ–Ω–∞
        for match in re.finditer(name_pattern, text):
            name_tokens = match.group(0).split()
            # –§–∏–ª—å—Ç—Ä—É–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã
            if not self._contains_legal_form(name_tokens):
                found_names.append(name_tokens)

        # –õ–∞—Ç–∏–Ω—Å–∫–∏–µ –∏–º–µ–Ω–∞
        for match in re.finditer(latin_name_pattern, text):
            name_tokens = match.group(0).split()
            if not self._contains_legal_form(name_tokens):
                found_names.append(name_tokens)

        return found_names

    def _extract_organization_tokens(self, text: str, language: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞.

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π.
        """
        from ...data.patterns.legal_forms import get_legal_forms_regex

        # –ò—â–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã –≤ —Ç–µ–∫—Å—Ç–µ
        legal_forms_regex = get_legal_forms_regex()
        organizations = []

        for match in re.finditer(legal_forms_regex, text, re.IGNORECASE):
            # –ù–∞–π–¥–µ–Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞, —Ç–µ–ø–µ—Ä—å –∏—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            org_text = self._extract_org_name_near_legal_form(text, match)
            if org_text:
                organizations.append(org_text.strip().upper())

        return list(set(organizations))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    def _contains_legal_form(self, tokens: List[str]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏ —Ç–æ–∫–µ–Ω—ã —é—Ä–∏–¥–∏—á–µ—Å–∫—É—é —Ñ–æ—Ä–º—É"""
        from ...data.patterns.legal_forms import get_legal_forms_set

        legal_forms = get_legal_forms_set()
        for token in tokens:
            if token.upper() in legal_forms:
                return True
        return False

    def _extract_org_name_near_legal_form(self, text: str, legal_form_match) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Ä—è–¥–æ–º —Å –Ω–∞–π–¥–µ–Ω–Ω–æ–π —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º–æ–π.

        –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ª–æ–≥–∏–∫–∏ –∏–∑ _extract_legal_forms.
        """
        legal_form = legal_form_match.group(0)
        start = legal_form_match.start()
        end = legal_form_match.end()

        # –ò—â–µ–º –∫–∞–≤—ã—á–∫–∏ —Ä—è–¥–æ–º —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º–æ–π
        quoted_pattern = r'["\u201c\u201d\u00ab\u00bb]([^"\u201c\u201d\u00ab\u00bb]+)["\u201c\u201d\u00ab\u00bb]'

        # –ò—â–µ–º –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º—ã (¬±100 —Å–∏–º–≤–æ–ª–æ–≤)
        search_start = max(0, start - 100)
        search_end = min(len(text), end + 100)
        search_area = text[search_start:search_end]

        quoted_matches = list(re.finditer(quoted_pattern, search_area))

        if quoted_matches:
            # –ë–µ—Ä–µ–º –±–ª–∏–∂–∞–π—à–µ–µ –∫–∞–≤—ã—á–∫–∏ –∫ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º–µ
            legal_form_pos_in_area = start - search_start
            closest_match = min(
                quoted_matches, key=lambda m: abs(m.start() - legal_form_pos_in_area)
            )
            return closest_match.group(1)

        # –ï—Å–ª–∏ –∫–∞–≤—ã—á–µ–∫ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–≥–ª—É—à–∫—É
        return "UNNAMED_ORG"

    # ==================== ASYNC METHODS ====================

    async def extract_async(
        self,
        text: str,
        normalization_result: Optional[Dict[str, Any]] = None,
        language: str = "uk",
    ) -> Dict[str, Any]:
        """
        Async version of extract using thread pool executor
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            normalization_result: –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã NormalizationService (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π)
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞

        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏:
            - persons: List[PersonSignal]
            - organizations: List[OrganizationSignal]
            - extras: Dict —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å–∏–≥–Ω–∞–ª—ã
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,  # Use default thread pool executor
            self.extract,
            text, normalization_result, language
        )
    
    def _extract_ids_from_normalization_trace(
        self, normalization_result: Optional[Dict[str, Any]], text: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –∏–∑ trace –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

        Args:
            normalization_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å trace
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–π

        Returns:
            Dict —Å –∫–ª—é—á–∞–º–∏ 'person_ids' –∏ 'organization_ids'
        """
        # Import INN validation for proper Russian/Ukrainian validation
        from ...data.patterns.identifiers import validate_inn
        
        if not normalization_result or 'trace' not in normalization_result:
            self.logger.debug("[CHECK] ID TRACE: No trace in normalization_result")
            return {'person_ids': [], 'organization_ids': []}

        trace = normalization_result['trace']
        person_ids = []
        organization_ids = []

        self.logger.debug(f"[CHECK] ID TRACE: Processing {len(trace)} trace entries")

        for entry in trace:
            # –ò—â–µ–º —Ç–æ–∫–µ–Ω—ã —Å —Ä–æ–ª—å—é 'id'
            if entry.get('role') == 'id' or entry.get('type') == 'role' and entry.get('role') == 'id':
                token_text = entry.get('token', '')

                if token_text and token_text.isdigit():
                    # –ù–∞–π–¥–µ–º –ø–æ–∑–∏—Ü–∏—é —Ç–æ–∫–µ–Ω–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                    import re
                    matches = list(re.finditer(re.escape(token_text), text))
                    position = matches[0].span() if matches else (0, len(token_text))

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø ID –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    id_length = len(token_text)

                    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π ID –¥–ª—è –ø–µ—Ä—Å–æ–Ω –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
                    id_info = {
                        "type": "numeric_id",  # –û–±—â–∏–π —Ç–∏–ø –¥–ª—è –≤—Å–µ—Ö numeric ID –∏–∑ trace
                        "value": token_text,
                        "raw": token_text,
                        "name": f"Numeric ID ({id_length} digits)",
                        "confidence": 0.95,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –Ω–∞–π–¥–µ–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
                        "position": position,
                        "valid": True,
                        "source": "normalization_trace"  # –û—Ç–º–µ—Ç–∫–∞ —á—Ç–æ –∏–∑ trace
                    }

                    # –î–æ–±–∞–≤–ª—è–µ–º –∏ –≤ person_ids –∏ –≤ organization_ids
                    # —Ç–∞–∫ –∫–∞–∫ –∏–∑ trace –Ω–µ—è—Å–Ω–æ –∫ —á–µ–º—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è ID
                    person_ids.append(id_info.copy())
                    organization_ids.append(id_info.copy())

                    self.logger.debug(f"[CHECK] ID TRACE: Found numeric ID '{token_text}' in trace")
            
            # –ò–©–ï–ú –ò–ù–ù –í NOTES - —ç—Ç–æ —Ñ–∏–∫—Å –¥–ª—è –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–≥–¥–∞ –ò–ù–ù –æ—Ç—Å–µ–∫–∞–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
            notes = entry.get('notes', '')
            if 'marker_–∏–Ω–Ω_nearby' in notes or 'marker_inn_nearby' in notes:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ò–ù–ù –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                token_text = entry.get('token', '')
                if token_text and token_text.isdigit() and len(token_text) >= 10:
                    # –ò—â–µ–º –ò–ù–ù –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ —Ä—è–¥–æ–º —Å —ç—Ç–∏–º —Ç–æ–∫–µ–Ω–æ–º
                    import re
                    inn_pattern = r'(?:(?:–ò–ù–ù|–∏–Ω–Ω|INN)\s*[\:\:]?\s*)?(\d{10,12})'
                    inn_matches = list(re.finditer(inn_pattern, text))

                    inn_found = False
                    for match in inn_matches:
                        inn_value = match.group(1)
                        if inn_value == token_text or len(token_text) == 10:  # –ò–ù–ù 2839403975 –∏–º–µ–µ—Ç 10 —Ü–∏—Ñ—Ä
                            position = match.span(1)  # –ü–æ–∑–∏—Ü–∏—è —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä

                            # –°–æ–∑–¥–∞–µ–º ID –¥–ª—è –ò–ù–ù —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
                            is_valid = validate_inn(inn_value)
                            inn_id_info = {
                                "type": "inn",
                                "value": inn_value,
                                "raw": match.group(0),  # –í–µ—Å—å –º–∞—Ç—á –≤–∫–ª—é—á–∞—è "–ò–ù–ù"
                                "name": "Taxpayer ID (INN)",
                                "confidence": 0.9 if is_valid else 0.6,
                                "position": position,
                                "valid": is_valid,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é RU + UA
                                "source": "normalization_trace_inn"
                            }

                            person_ids.append(inn_id_info.copy())
                            self.logger.warning(f"[CHECK] ID TRACE: Found INN '{inn_value}' from marker_–∏–Ω–Ω_nearby in trace (valid={is_valid})")
                            inn_found = True
                            break

                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ò–ù–ù pattern, –ù–ï –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ numeric_id
                    # (—ç—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ç–∏–ø–∏–∑–∞—Ü–∏—é)
                    if not inn_found:
                        self.logger.debug(f"[CHECK] ID TRACE: Token '{token_text}' with marker_–∏–Ω–Ω_nearby but no INN pattern match")

        self.logger.debug(f"[CHECK] ID TRACE: Extracted {len(person_ids)} person IDs, {len(organization_ids)} org IDs from trace")
        return {'person_ids': person_ids, 'organization_ids': organization_ids}

    def _deduplicate_ids(self, ids: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã ID –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é, —Å–æ—Ö—Ä–∞–Ω—è—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç trace > regex.

        Args:
            ids: –°–ø–∏—Å–æ–∫ ID –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID
        """
        if not ids:
            return []

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é
        id_groups = {}
        for id_info in ids:
            value = id_info.get('value', '')
            if value not in id_groups:
                id_groups[value] = []
            id_groups[value].append(id_info)

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π ID –∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã (INN extractor > trace > other regex)
        unique_ids = []
        for value, group in id_groups.items():
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º: INN —Ç–∏–ø—ã –ø–µ—Ä–≤—ã–º–∏, –ø–æ—Ç–æ–º trace, –ø–æ—Ç–æ–º –ø–æ confidence
            group.sort(key=lambda x: (
                x.get('type') not in ['inn', 'inn_ua', 'inn_ru'],  # INN —Ç–∏–ø—ã –ø–µ—Ä–≤—ã–µ (False < True)
                x.get('source') != 'normalization_trace',  # trace –≤—Ç–æ—Ä—ã–µ (False < True)  
                -x.get('confidence', 0)  # –ø–æ—Ç–æ–º –ø–æ —É–±—ã–≤–∞—é—â–µ–π confidence
            ))

            # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π
            best_id = group[0]
            unique_ids.append(best_id)

            self.logger.debug(f"[CHECK] ID DEDUP: Selected {best_id.get('source', 'regex')} source for ID '{value}'")

        return unique_ids

    def _check_sanctioned_inn_cache(
        self,
        person_ids: List[Dict[str, Any]],
        org_ids: List[Dict[str, Any]],
        persons: List[PersonSignal],
        organizations: List[OrganizationSignal]
    ):
        """
        FAST PATH: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç ID –≤ sanctioned INN cache –∏ —Å—Ä–∞–∑—É –ø–æ–º–µ—á–∞–µ—Ç –∫–∞–∫ matched.

        Args:
            person_ids: –°–ø–∏—Å–æ–∫ ID –ø–µ—Ä—Å–æ–Ω
            org_ids: –°–ø–∏—Å–æ–∫ ID –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
            persons: –°–ø–∏—Å–æ–∫ –ø–µ—Ä—Å–æ–Ω –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
            organizations: –°–ø–∏—Å–æ–∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è
        """
        try:
            from ...layers.search.sanctioned_inn_cache import get_inn_cache
            inn_cache = get_inn_cache()

            # Try to get metrics exporter, but don't fail if not available
            metrics = None
            try:
                from ...monitoring.prometheus_exporter import get_exporter
                metrics = get_exporter()
            except Exception:
                # Metrics not available, continue without them
                pass

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ ID –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            all_ids_to_check = []

            # –î–æ–±–∞–≤–ª—è–µ–º person IDs —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
            self.logger.warning(f"[CHECK] FAST PATH INPUT: Processing {len(person_ids)} person IDs")
            for idx, id_info in enumerate(person_ids):
                id_value = id_info.get('value', '')
                id_type = id_info.get('type', '')
                is_valid = id_info.get('valid', None)
                id_source = id_info.get('source', 'unknown')

                self.logger.warning(
                    f"[CHECK] FAST PATH [{idx+1}/{len(person_ids)}]: "
                    f"value='{id_value}' type='{id_type}' valid={is_valid} source={id_source}"
                )

                if id_value and id_value.isdigit():
                    # –î–ª—è –ò–ù–ù –ø—Ä–æ–≤–µ—Ä—è–µ–º –í–°–ï –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                    # –ö–†–ò–¢–ò–ß–ù–û: –î–∞–∂–µ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π –ò–ù–ù –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã—Ö —Å–ø–∏—Å–∫–∞—Ö!
                    if id_type == 'inn' and len(id_value) in [10, 12]:
                        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—Ä–æ–≤–µ—Ä–∫—É –í–°–ï–ì–î–ê, –¥–∞–∂–µ –µ—Å–ª–∏ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π
                        all_ids_to_check.append((id_value, 'person', id_info))
                        self.logger.warning(
                            f"[OK] FAST PATH: Added INN for sanction check: {id_value} "
                            f"(type: {id_type}, valid={is_valid}, will check anyway)"
                        )
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∏–ø–æ–≤ ID –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–µ
                    elif len(id_value) >= 10 and id_info.get('valid', True):
                        all_ids_to_check.append((id_value, 'person', id_info))
                        self.logger.debug(f"[INIT] FAST PATH: Added valid ID for sanction check: {id_value} (type: {id_type})")
                    else:
                        self.logger.warning(
                            f"[WARN] FAST PATH SKIP: ID '{id_value}' not added "
                            f"(type={id_type}, len={len(id_value)}, valid={is_valid})"
                        )

            # –î–æ–±–∞–≤–ª—è–µ–º org IDs
            for id_info in org_ids:
                id_value = id_info.get('value', '')
                if id_value and id_value.isdigit() and len(id_value) >= 8:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –Ñ–î–†–ü–û–£
                    all_ids_to_check.append((id_value, 'org', id_info))

            if not all_ids_to_check:
                return

            self.logger.warning(f"[INIT] FAST PATH: Checking {len(all_ids_to_check)} IDs against sanctions cache")
            if all_ids_to_check:
                self.logger.warning(f"[INIT] FAST PATH: IDs to check: {[(id_value, entity_type, id_info.get('type', 'unknown')) for id_value, entity_type, id_info in all_ids_to_check[:5]]}")
            else:
                self.logger.warning("[INIT] FAST PATH: No IDs to check - this is the problem!")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π ID –≤ cache
            sanctioned_matches = 0
            for id_value, entity_type, id_info in all_ids_to_check:
                sanctioned_data = inn_cache.lookup(id_value)

                # Record cache lookup metrics
                cache_hit = sanctioned_data is not None
                if metrics:
                    metrics.record_fast_path_cache_lookup(cache_hit)

                if sanctioned_data:
                    sanctioned_matches += 1
                    self.logger.warning(
                        f"üö® FAST PATH SANCTION HIT: {id_value} -> {sanctioned_data.get('name', 'Unknown')} "
                        f"(type: {sanctioned_data.get('type', 'unknown')})"
                    )

                    # –û–±–æ–≥–∞—â–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏ —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
                    if entity_type == 'person' and persons:
                        self._enrich_person_with_sanctioned_data(persons[0], sanctioned_data, id_info)
                    elif entity_type == 'org' and organizations:
                        self._enrich_organization_with_sanctioned_data(organizations[0], sanctioned_data, id_info)

            if sanctioned_matches > 0:
                self.logger.warning(f"üö® FAST PATH: Found {sanctioned_matches} sanctioned ID matches in cache")
            else:
                self.logger.debug("[OK] FAST PATH: No sanctions found in INN cache")

        except ImportError:
            self.logger.warning("INN cache not available - falling back to regular search")
        except Exception as e:
            self.logger.error(f"Error in sanctioned INN cache check: {e}")

    def _enrich_person_with_sanctioned_data(
        self, person: PersonSignal, sanctioned_data: Dict[str, Any], id_info: Dict[str, Any]
    ):
        """–û–±–æ–≥–∞—â–∞–µ—Ç –ø–µ—Ä—Å–æ–Ω—É —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ cache."""
        id_value = id_info.get('value')

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–π ID —É –ø–µ—Ä—Å–æ–Ω—ã
        existing_id = None
        for existing in person.ids:
            if existing.get('value') == id_value:
                existing_id = existing
                break

        if existing_id:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π ID —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            existing_id['sanctioned'] = True
            existing_id['sanctioned_name'] = sanctioned_data.get('name')
            existing_id['sanctioned_source'] = sanctioned_data.get('source', 'sanctions_cache')
            existing_id['confidence'] = 1.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            self.logger.warning(f"üö® UPDATED existing ID {id_value} with sanctioned flag")
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π ID –∫ –ø–µ—Ä—Å–æ–Ω–µ —Å —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–æ–π –ø–æ–º–µ—Ç–∫–æ–π
            sanctioned_id = {
                **id_info,
                'sanctioned': True,
                'sanctioned_name': sanctioned_data.get('name'),
                'sanctioned_source': sanctioned_data.get('source', 'sanctions_cache'),
                'confidence': 1.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            }
            person.ids.append(sanctioned_id)
            self.logger.warning(f"üö® ADDED new sanctioned ID {id_value}")

        # –û–±–æ–≥–∞—â–∞–µ–º evidence
        person.evidence.append(f"sanctioned_inn_cache_hit_{id_value}")

        # –ü–æ–≤—ã—à–∞–µ–º confidence –ø–µ—Ä—Å–æ–Ω—ã
        person.confidence = max(person.confidence, 0.95)

        self.logger.warning(f"üö® Enriched person '{person.full_name}' with sanctioned INN {id_value} -> {sanctioned_data.get('name')}")

    def _enrich_organization_with_sanctioned_data(
        self, org: OrganizationSignal, sanctioned_data: Dict[str, Any], id_info: Dict[str, Any]
    ):
        """–û–±–æ–≥–∞—â–∞–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–∑ cache."""
        # –î–æ–±–∞–≤–ª—è–µ–º ID –∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω–æ–π –ø–æ–º–µ—Ç–∫–æ–π
        sanctioned_id = {
            **id_info,
            'sanctioned': True,
            'sanctioned_name': sanctioned_data.get('name'),
            'sanctioned_source': sanctioned_data.get('source', 'sanctions_cache'),
            'confidence': 1.0  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        }

        org.ids.append(sanctioned_id)

        # –û–±–æ–≥–∞—â–∞–µ–º evidence
        org.evidence.append(f"sanctioned_inn_cache_hit_{id_info.get('value')}")

        # –ü–æ–≤—ã—à–∞–µ–º confidence –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        org.confidence = max(org.confidence, 0.95)

        self.logger.debug(f"Enriched organization '{org.core}' with sanctioned INN data")

    def _is_mixed_language_text(self, text: str) -> bool:
        """
        Check if text contains mixed language (both Cyrillic and Latin scripts)
        
        Args:
            text: Text to analyze
            
        Returns:
            True if text contains both Cyrillic and Latin characters
        """
        if not text:
            return False
            
        # Count Cyrillic characters
        cyrillic_count = len(re.findall(r"[–∞-—è—ë—ñ—ó—î“ë]", text, re.IGNORECASE))
        
        # Count Latin characters
        latin_count = len(re.findall(r"[a-z]", text, re.IGNORECASE))
        
        # Consider mixed if both scripts are present
        return cyrillic_count > 0 and latin_count > 0
