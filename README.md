# ðŸš€ AI Service - Unified Architecture

**Clean, consolidated AI service** for normalization and structured extraction from sanctions data with support for multiple languages (English, Russian, Ukrainian).

## âœ¨ **Unified Architecture Benefits**

ðŸŽ¯ **Single Orchestrator** - Replaced 3+ duplicate implementations with one clean `UnifiedOrchestrator`
ðŸ“‹ **CLAUDE.md Compliant** - Exact 9-layer specification implementation
ðŸ” **Structured Signals** - Persons, organizations, IDs, dates with full traceability
âš¡ **Performance Optimized** - â‰¤10ms for short strings, comprehensive caching
ðŸ§ª **Comprehensive Testing** - 12 real payment scenarios, contract validation

## ðŸ—ï¸ **9-Layer Architecture**

```
1. Validation & Sanitization  â†’  Basic input validation
2. Smart Filter              â†’  Pre-processing decision
3. Language Detection        â†’  ru/uk/en identification
4. Unicode Normalization     â†’  Text standardization
5. Name Normalization (CORE) â†’  Person names + org cores
6. Signals                   â†’  Structured extraction
7. Variants (optional)       â†’  Spelling alternatives
8. Embeddings (optional)     â†’  Vector representation
9. Decision & Response       â†’  Final result assembly
```

## ðŸŽ¯ **Core Features**

- **ðŸ“ Text Normalization**: Morphological analysis with token-level tracing
- **ðŸ¢ Structured Extraction**: Persons with DOB/IDs, organizations with legal forms
- **ðŸŒ Multi-language Support**: Russian, Ukrainian, English with mixed-script handling
- **ðŸ” Smart Filtering**: Pre-processing optimization with signal detection
- **ðŸ“Š Signal Analysis**: Legal forms, payment contexts, document numbers
- **ðŸŽ¯ Variant Generation**: Transliteration, phonetic, morphological variants
- **âš¡ High Performance**: Caching, async processing, performance monitoring

## ðŸ”® **Embeddings**

The EmbeddingService provides pure vector generation capabilities using multilingual sentence transformers:

### **Vector Generation Only**
- **`encode_one(text: str) -> List[float]`**: Generate 384-dimensional vector for single text
- **`encode_batch(texts: List[str]) -> List[List[float]]`**: Generate vectors for multiple texts
- **Default Model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Output**: 32-bit float vectors, normalized and ready for downstream processing

### **Configuration**
```python
from ai_service.config import config
from ai_service.layers.embeddings.embedding_service import EmbeddingService

# Use default configuration
service = EmbeddingService(config.embedding)

# Generate vectors
vector = service.encode_one("Ivan Ivanov")  # 384 floats
vectors = service.encode_batch(["A", "B"])  # 2x384 floats
```

### **Important Notes**
- **No Indexing**: This service only generates vectors - indexing is handled downstream
- **No Similarity Search**: Vector similarity calculations are done by other services
- **Lazy Loading**: Models are loaded only when first needed
- **Batch Processing**: Optimized for processing multiple texts efficiently

## Quick Start

### Prerequisites

- Python 3.12+
- Poetry for dependency management
- Docker (optional)

### Local Development

1. **Install dependencies**:
   ```bash
   make install-dev
   ```

2. **Start the service**:
   ```bash
   make start
   ```

3. **Run tests**:
   ```bash
   make test
   ```

### Docker Deployment

1. **Build the image**:
   ```bash
   make docker-build
   ```

2. **Run production container**:
   ```bash
   make docker-run
   ```

3. **Run development container**:
   ```bash
   make docker-dev
   ```

## API Endpoints

### Core Endpoints

- `POST /process` - Complete text processing
- `POST /normalize` - Text normalization
- `POST /process-batch` - Batch text processing
- `POST /search-similar` - Similarity search
- `POST /analyze-complexity` - Text complexity analysis

### Administrative Endpoints (Protected)

- `POST /clear-cache` - Clear cache (requires API key)
- `POST /reset-stats` - Reset statistics (requires API key)

### Information Endpoints

- `GET /health` - Service health check
- `GET /stats` - Processing statistics
- `GET /languages` - Supported languages
- `GET /` - Service information

## Configuration

The service uses environment variables for configuration:

- `APP_ENV`: Environment (development, staging, production)
- `WORKERS`: Number of worker processes (production only)
- `DEBUG`: Enable debug mode

### Security

Administrative endpoints are protected with API key authentication. Set the key in `config.py`:

```python
SECURITY_CONFIG = {
    'admin_api_key': 'your-secure-api-key-here'
}
```

## Development

### Project Structure

```
src/ai_service/
â”œâ”€â”€ services/           # Core services
â”‚   â”œâ”€â”€ orchestrator_service.py
â”‚   â”œâ”€â”€ normalization_service.py
â”‚   â”œâ”€â”€ variant_generation_service.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/              # Data files and templates
â”œâ”€â”€ config/            # Configuration files
â””â”€â”€ utils/             # Utility functions

tests/
â”œâ”€â”€ unit/              # Unit tests
â”œâ”€â”€ integration/       # Integration tests
â””â”€â”€ e2e/              # End-to-end tests
```

### Running Tests

```bash
# All tests
make test

# With coverage
make test-cov

# Specific test file
poetry run pytest tests/unit/test_orchestrator_service.py -v
```

### Code Quality

```bash
# Format code
make format

# Lint code
make lint

# Clean up
make clean
```

## Deployment

### Production

1. **Set environment**:
   ```bash
   export APP_ENV=production
   export WORKERS=4
   ```

2. **Install dependencies**:
   ```bash
   make install
   ```

3. **Start service**:
   ```bash
   make start-prod
   ```

### Docker Compose

```bash
# Production
docker-compose up ai-service

# Development
docker-compose --profile dev up ai-service-dev
```

## CI/CD

The project includes GitHub Actions workflow for automated testing:

- Runs on push to `main` and `develop` branches
- Runs on pull requests
- Installs dependencies with Poetry
- Runs tests with coverage
- Uploads coverage to Codecov

## Monitoring

### Health Check

```bash
curl http://localhost:8000/health
```

### Statistics

```bash
curl http://localhost:8000/stats
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass
6. Submit a pull request

@ Daria Pavlova