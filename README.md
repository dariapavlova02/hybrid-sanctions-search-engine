# ğŸš€ AI Service - Unified Architecture

**Clean, consolidated AI service** for normalization and structured extraction from sanctions data with support for multiple languages (English, Russian, Ukrainian).

## âœ¨ **Unified Architecture Benefits**

ğŸ¯ **Single Orchestrator** - Replaced 3+ duplicate implementations with one clean `UnifiedOrchestrator`
ğŸ“‹ **CLAUDE.md Compliant** - Exact 9-layer specification implementation
ğŸ” **Structured Signals** - Persons, organizations, IDs, dates with full traceability
âš¡ **Performance Optimized** - â‰¤10ms for short strings, comprehensive caching
ğŸ§ª **Comprehensive Testing** - 12 real payment scenarios, contract validation

## ğŸ—ï¸ **9-Layer Architecture**

```
1. Validation & Sanitization  â†’  Basic input validation
2. Smart Filter              â†’  Pre-processing decision
3. Language Detection        â†’  ru/uk/en identification
4. Unicode Normalization     â†’  Text standardization
5. Name Normalization (CORE) â†’  Person names + org cores
6. Signals                   â†’  Structured extraction
7. Variants (optional)       â†’  Spelling alternatives
8. Embeddings (optional)     â†’  Vector representation
9. Decision & Response       â†’  Final result assembly
```

## ğŸ¯ **Core Features**

- **ğŸ“ Text Normalization**: Morphological analysis with token-level tracing
- **ğŸ¢ Structured Extraction**: Persons with DOB/IDs, organizations with legal forms
- **ğŸŒ Multi-language Support**: Russian, Ukrainian, English with mixed-script handling
- **ğŸ” Smart Filtering**: Pre-processing optimization with signal detection
- **ğŸ“Š Signal Analysis**: Legal forms, payment contexts, document numbers
- **ğŸ¯ Variant Generation**: Transliteration, phonetic, morphological variants
- **âš¡ High Performance**: Caching, async processing, performance monitoring

## ğŸ”® **Embeddings**

The EmbeddingService provides pure vector generation capabilities using multilingual sentence transformers:

### **Model Choice & Architecture**
- **Default Model**: `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
- **Why L12 v2**: Balanced performance (384-dim) with multilingual support (ru/uk/en)
- **Output**: 32-bit float vectors, ready for downstream similarity analysis
- **Preprocessing**: Automatically removes dates/IDs - only names/organizations are embedded

### **Why Dates/IDs Are Excluded**
- **Separation of Concerns**: Names â†’ semantic similarity, Dates/IDs â†’ exact matching
- **Downstream Processing**: Signals layer handles structured data, Decision layer does exact matching
- **Performance**: Cleaner embeddings without noise from structured data

### **API Usage**
```python
from ai_service.config import EmbeddingConfig
from ai_service.layers.embeddings.embedding_service import EmbeddingService

# Initialize service
config = EmbeddingConfig()
service = EmbeddingService(config)

# Single text encoding
vector = service.encode_one("Ivan Petrov")  # 384 floats

# Batch encoding (recommended)
vectors = service.encode_batch(["Ivan Petrov", "Anna Smith"])  # 2x384 floats

# Multilingual support
ru_vector = service.encode_one("Ğ˜Ğ²Ğ°Ğ½ ĞŸĞµÑ‚Ñ€Ğ¾Ğ²")
uk_vector = service.encode_one("Ğ†Ğ²Ğ°Ğ½ ĞŸĞµÑ‚Ñ€Ğ¾Ğ²") 
en_vector = service.encode_one("Ivan Petrov")
# All vectors are comparable for similarity analysis
```

### **Model Switching**
```python
# Switch models via configuration
config = EmbeddingConfig(
    model_name="sentence-transformers/all-MiniLM-L6-v2",
    extra_models=["sentence-transformers/all-MiniLM-L6-v2"]
)
service = EmbeddingService(config)
```

### **Performance SLA**
- **Latency**: < 15ms single text, < 200ms for 1000 texts (p95)
- **Memory**: ~80MB model + ~1MB per 100 texts
- **Throughput**: ~100 texts/sec (CPU), ~500 texts/sec (GPU)

### **Important Notes**
- **No Indexing**: Pure vector generation - indexing handled downstream
- **No Similarity Search**: Vector similarity calculations done by other services  
- **Lazy Loading**: Models loaded only when first needed
- **Batch Processing**: Optimized for processing multiple texts efficiently

ğŸ“– **Detailed Documentation**: See [docs/embeddings.md](docs/embeddings.md) for complete usage guide

## Quick Start

### Prerequisites

- Python 3.12+
- Poetry for dependency management
- Docker (optional)

### Local Development

1. **Install dependencies**:
   ```bash
   make install-dev
   ```

2. **Start the service**:
   ```bash
   make start
   ```

3. **Run tests**:
   ```bash
   make test
   ```

### Docker Deployment

1. **Build the image**:
   ```bash
   make docker-build
   ```

2. **Run production container**:
   ```bash
   make docker-run
   ```

3. **Run development container**:
   ```bash
   make docker-dev
   ```

## API Endpoints

### Core Endpoints

- `POST /process` - Complete text processing
- `POST /normalize` - Text normalization
- `POST /process-batch` - Batch text processing
- `POST /search-similar` - Similarity search
- `POST /analyze-complexity` - Text complexity analysis

### Administrative Endpoints (Protected)

- `POST /clear-cache` - Clear cache (requires API key)
- `POST /reset-stats` - Reset statistics (requires API key)

### Information Endpoints

- `GET /health` - Service health check
- `GET /stats` - Processing statistics
- `GET /languages` - Supported languages
- `GET /` - Service information

## Configuration

The service uses environment variables for configuration:

- `APP_ENV`: Environment (development, staging, production)
- `WORKERS`: Number of worker processes (production only)
- `DEBUG`: Enable debug mode

### Security

Administrative endpoints are protected with API key authentication. Set the key in `config.py`:

```python
SECURITY_CONFIG = {
    'admin_api_key': 'your-secure-api-key-here'
}
```

## Development

### Project Structure

```
src/ai_service/
â”œâ”€â”€ services/           # Core services
â”‚   â”œâ”€â”€ orchestrator_service.py
â”‚   â”œâ”€â”€ normalization_service.py
â”‚   â”œâ”€â”€ variant_generation_service.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/              # Data files and templates
â”œâ”€â”€ config/            # Configuration files
â””â”€â”€ utils/             # Utility functions

tests/
â”œâ”€â”€ unit/              # Unit tests
â”œâ”€â”€ integration/       # Integration tests
â””â”€â”€ e2e/              # End-to-end tests
```

### Running Tests

```bash
# All tests
make test

# With coverage
make test-cov

# Specific test file
poetry run pytest tests/unit/test_orchestrator_service.py -v
```

### Code Quality

```bash
# Format code
make format

# Lint code
make lint

# Clean up
make clean
```

## Deployment

### Production

1. **Set environment**:
   ```bash
   export APP_ENV=production
   export WORKERS=4
   ```

2. **Install dependencies**:
   ```bash
   make install
   ```

3. **Start service**:
   ```bash
   make start-prod
   ```

### Docker Compose

```bash
# Production
docker-compose up ai-service

# Development
docker-compose --profile dev up ai-service-dev
```

## CI/CD

The project includes GitHub Actions workflow for automated testing:

- Runs on push to `main` and `develop` branches
- Runs on pull requests
- Installs dependencies with Poetry
- Runs tests with coverage
- Uploads coverage to Codecov

## Monitoring

### Health Check

```bash
curl http://localhost:8000/health
```

### Statistics

```bash
curl http://localhost:8000/stats
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Ensure all tests pass
6. Submit a pull request

@ Daria Pavlova