#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫
"""

import json
import os
import sys
import re
from pathlib import Path
from typing import Dict, List, Set, Any
from collections import Counter

# Add path to src for imports
sys.path.append(str(Path(__file__).parent / "src"))

from ai_service.utils.logging_config import get_logger


class AhoCorasickPatternFixer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫"""
    
    def __init__(self):
        self.logger = get_logger(__name__)
        
    def load_patterns(self, patterns_file: str) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(patterns_file, 'r', encoding='utf-8') as f:
                return [line.strip() for line in f if line.strip()]
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {patterns_file}: {e}")
            return []
    
    def save_patterns(self, patterns: List[str], output_file: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ñ–∞–π–ª"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                for pattern in patterns:
                    f.write(pattern + '\n')
            self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ {output_file}")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ {output_file}: {e}")
    
    def remove_duplicates(self, patterns: List[str]) -> List[str]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫"""
        seen = set()
        unique_patterns = []
        
        for pattern in patterns:
            if pattern not in seen:
                seen.add(pattern)
                unique_patterns.append(pattern)
        
        removed_count = len(patterns) - len(unique_patterns)
        self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed_count} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        
        return unique_patterns
    
    def fix_whitespace_issues(self, patterns: List[str]) -> List[str]:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–±–µ–ª–∞–º–∏"""
        fixed_patterns = []
        issues_fixed = 0
        
        for pattern in patterns:
            # –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –∏ –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ –ø—Ä–æ–±–µ–ª—ã
            pattern = pattern.strip()
            
            # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ
            pattern = re.sub(r'\s+', ' ', pattern)
            
            # –£–¥–∞–ª—è–µ–º —Ç–∞–±—É–ª—è—Ü–∏–∏ –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫
            pattern = pattern.replace('\t', ' ').replace('\n', ' ').replace('\r', ' ')
            
            if pattern != patterns[len(fixed_patterns)]:
                issues_fixed += 1
            
            fixed_patterns.append(pattern)
        
        self.logger.info(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {issues_fixed} –ø—Ä–æ–±–ª–µ–º —Å –ø—Ä–æ–±–µ–ª–∞–º–∏")
        return fixed_patterns
    
    def fix_case_issues(self, patterns: List[str]) -> List[str]:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º"""
        fixed_patterns = []
        issues_fixed = 0
        
        for pattern in patterns:
            original_pattern = pattern
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
            words = pattern.split()
            fixed_words = []
            
            for word in words:
                if len(word) == 0:
                    continue
                
                # –ï—Å–ª–∏ —Å–ª–æ–≤–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–º–µ—à–∞–Ω–Ω—ã–π —Ä–µ–≥–∏—Å—Ç—Ä, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                if word[0].islower() and any(c.isupper() for c in word[1:]):
                    # –î–µ–ª–∞–µ–º –ø–µ—Ä–≤—É—é –±—É–∫–≤—É –∑–∞–≥–ª–∞–≤–Ω–æ–π, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–æ—á–Ω—ã–º–∏
                    word = word[0].upper() + word[1:].lower()
                    issues_fixed += 1
                elif word.isupper() and len(word) > 1:
                    # –ï—Å–ª–∏ –≤—Å–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ, –¥–µ–ª–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –∑–∞–≥–ª–∞–≤–Ω–æ–π
                    word = word[0].upper() + word[1:].lower()
                    issues_fixed += 1
            
            fixed_pattern = ' '.join(fixed_words)
            fixed_patterns.append(fixed_pattern)
        
        self.logger.info(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {issues_fixed} –ø—Ä–æ–±–ª–µ–º —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º")
        return fixed_patterns
    
    def remove_short_patterns(self, patterns: List[str], min_length: int = 3) -> List[str]:
        """–£–¥–∞–ª—è–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        filtered_patterns = []
        removed_count = 0
        
        for pattern in patterns:
            if len(pattern.strip()) >= min_length:
                filtered_patterns.append(pattern)
            else:
                removed_count += 1
        
        self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed_count} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (< {min_length} —Å–∏–º–≤–æ–ª–æ–≤)")
        return filtered_patterns
    
    def remove_very_long_patterns(self, patterns: List[str], max_length: int = 200) -> List[str]:
        """–£–¥–∞–ª—è–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        filtered_patterns = []
        removed_count = 0
        
        for pattern in patterns:
            if len(pattern.strip()) <= max_length:
                filtered_patterns.append(pattern)
            else:
                removed_count += 1
        
        self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed_count} —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (> {max_length} —Å–∏–º–≤–æ–ª–æ–≤)")
        return filtered_patterns
    
    def fix_encoding_issues(self, patterns: List[str]) -> List[str]:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π"""
        fixed_patterns = []
        issues_fixed = 0
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        encoding_fixes = {
            '√¢': '–∞', '√¨': '—ñ', '√™': '–µ', 'ƒç': '—á', ' π': '—å',
            '√Ø': '—ñ', '√π': '—É', '√≤': '–æ', '√®': '–µ', '√†': '–∞',
            '√©': '–µ', '√≥': '–æ', '√∫': '—É', '√≠': '—ñ', '√±': '–Ω'
        }
        
        for pattern in patterns:
            original_pattern = pattern
            fixed_pattern = pattern
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            for wrong_char, correct_char in encoding_fixes.items():
                fixed_pattern = fixed_pattern.replace(wrong_char, correct_char)
            
            if fixed_pattern != original_pattern:
                issues_fixed += 1
            
            fixed_patterns.append(fixed_pattern)
        
        self.logger.info(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {issues_fixed} –ø—Ä–æ–±–ª–µ–º —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π")
        return fixed_patterns
    
    def remove_suspicious_patterns(self, patterns: List[str]) -> List[str]:
        """–£–¥–∞–ª—è–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        filtered_patterns = []
        removed_count = 0
        
        for pattern in patterns:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —á–∏—Å–ª–∞
            if re.match(r'^\d+$', pattern) and len(pattern) > 10:
                removed_count += 1
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤
            if re.match(r'^[a-zA-Z]+$', pattern) and len(pattern) > 50:
                removed_count += 1
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–æ–ª—å–∫–æ –∏–∑ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–∏–º–≤–æ–ª–æ–≤
            if len(set(pattern)) == 1 and len(pattern) > 2:
                removed_count += 1
                continue
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤
            if pattern.isspace():
                removed_count += 1
                continue
            
            filtered_patterns.append(pattern)
        
        self.logger.info(f"–£–¥–∞–ª–µ–Ω–æ {removed_count} –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        return filtered_patterns
    
    def normalize_patterns(self, patterns: List[str]) -> List[str]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        normalized_patterns = []
        
        for pattern in patterns:
            # –£–¥–∞–ª—è–µ–º –≤–µ–¥—É—â–∏–µ –∏ –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ –ø—Ä–æ–±–µ–ª—ã
            pattern = pattern.strip()
            
            # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ
            pattern = re.sub(r'\s+', ' ', pattern)
            
            # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if not pattern:
                continue
            
            normalized_patterns.append(pattern)
        
        return normalized_patterns
    
    def fix_all_issues(self, patterns: List[str]) -> List[str]:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã"""
        self.logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
        
        original_count = len(patterns)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ –ø–æ—Ä—è–¥–∫—É
        patterns = self.normalize_patterns(patterns)
        patterns = self.remove_duplicates(patterns)
        patterns = self.fix_whitespace_issues(patterns)
        patterns = self.fix_case_issues(patterns)
        patterns = self.fix_encoding_issues(patterns)
        patterns = self.remove_short_patterns(patterns, min_length=3)
        patterns = self.remove_very_long_patterns(patterns, max_length=200)
        patterns = self.remove_suspicious_patterns(patterns)
        
        final_count = len(patterns)
        removed_count = original_count - final_count
        
        self.logger.info(f"–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –£–¥–∞–ª–µ–Ω–æ {removed_count} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {original_count}")
        
        return patterns
    
    def generate_fix_report(self, original_patterns: List[str], fixed_patterns: List[str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö"""
        original_count = len(original_patterns)
        fixed_count = len(fixed_patterns)
        removed_count = original_count - fixed_count
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
        original_short = sum(1 for p in original_patterns if len(p.strip()) < 3)
        fixed_short = sum(1 for p in fixed_patterns if len(p.strip()) < 3)
        
        original_long = sum(1 for p in original_patterns if len(p.strip()) > 200)
        fixed_long = sum(1 for p in fixed_patterns if len(p.strip()) > 200)
        
        original_duplicates = len(original_patterns) - len(set(original_patterns))
        fixed_duplicates = len(fixed_patterns) - len(set(fixed_patterns))
        
        report = f"""
=== –û–¢–ß–ï–¢ –û–ë –ò–°–ü–†–ê–í–õ–ï–ù–ò–ò –ü–ê–¢–¢–ï–†–ù–û–í –ê–•–û-–ö–û–†–ê–°–ò–ö ===

üìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {original_count}
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {fixed_count}
- –£–¥–∞–ª–µ–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {removed_count}
- –ü—Ä–æ—Ü–µ–Ω—Ç —É–¥–∞–ª–µ–Ω–Ω—ã—Ö: {(removed_count / original_count * 100):.2f}%

üîç –î–ï–¢–ê–õ–¨–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø:
- –ö–æ—Ä–æ—Ç–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (< 3 —Å–∏–º–≤–æ–ª–æ–≤): {original_short} ‚Üí {fixed_short}
- –î–ª–∏–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (> 200 —Å–∏–º–≤–æ–ª–æ–≤): {original_long} ‚Üí {fixed_long}
- –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {original_duplicates} ‚Üí {fixed_duplicates}

‚úÖ –ü–†–ò–ú–ï–ù–ï–ù–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
2. –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å —Ä–µ–≥–∏—Å—Ç—Ä–æ–º
4. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
5. –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (< 3 —Å–∏–º–≤–æ–ª–æ–≤)
6. –£–¥–∞–ª–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (> 200 —Å–∏–º–≤–æ–ª–æ–≤)
7. –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤

üìà –†–ï–ó–£–õ–¨–¢–ê–¢:
- –ö–∞—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–æ
- –£–¥–∞–ª–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∏ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—Å–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –£–ª—É—á—à–µ–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ê—Ö–æ-–ö–æ—Ä–∞—Å–∏–∫

=== –ö–û–ù–ï–¶ –û–¢–ß–ï–¢–ê ===
"""
        
        return report


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    fixer = AhoCorasickPatternFixer()
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    input_file = "src/ai_service/data/templates/aho_corasick_patterns.txt"
    output_file = "src/ai_service/data/templates/aho_corasick_patterns_fixed.txt"
    backup_file = "src/ai_service/data/templates/aho_corasick_patterns_backup.txt"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    if not os.path.exists(input_file):
        print(f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã...")
    patterns = fixer.load_patterns(input_file)
    
    if not patterns:
        print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        return
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
    print("–°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é...")
    fixer.save_patterns(patterns, backup_file)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    print("–ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã...")
    fixed_patterns = fixer.fix_all_issues(patterns)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    print("–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã...")
    fixer.save_patterns(fixed_patterns, output_file)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
    print("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç...")
    report = fixer.generate_fix_report(patterns, fixed_patterns)
    print(report)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    with open("aho_corasick_patterns_fix_report.txt", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\n–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")
    print(f"–†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {backup_file}")
    print("–û—Ç—á–µ—Ç –æ–± –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: aho_corasick_patterns_fix_report.txt")


if __name__ == "__main__":
    main()
